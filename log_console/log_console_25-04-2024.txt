Global seed set to 2024
wandb: Currently logged in as: 01133344 (cccvb). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240425_042229-dqdqvdb8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_dla_latent_size_10_dqdqvdb8_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28/runs/dqdqvdb8
Experiments to be run:
* 
-d c100 --model.num_classes 100 --config.num_tasks 2 --loop.schedule 300 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 3 --loop.train_at True --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 2 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at False --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 440 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.001 --model.loss.chi.ratio_milestones 40 60 100 --loop.vis.layerloss.deep_inversion.scale 10 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type dla --model.latent.size 10
* 
-d c100 --model.num_classes 100 --config.num_tasks 2 --loop.schedule 300 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 3 --loop.train_at True --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 2 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at False --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 440 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.001 --model.loss.chi.ratio_milestones 40 60 100 --loop.vis.layerloss.deep_inversion.scale 10 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type vgg --model.latent.size 10
* 
-d c100 --model.num_classes 100 --config.num_tasks 2 --loop.schedule 300 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 3 --loop.train_at True --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 2 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at False --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 440 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.001 --model.loss.chi.ratio_milestones 40 60 100 --loop.vis.layerloss.deep_inversion.scale 10 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type custom-resnet34 --model.latent.size 10

Running experiment: exp_search_model_type_dla_latent_size_10; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Full second command line: [0m
-d c100 --model.num_classes 100 --config.num_tasks 2 --loop.schedule 300 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 3 --loop.train_at True --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 2 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at False --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam --datamodule.vis.image_type pixel --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 440 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.001 --model.loss.chi.ratio_milestones 40 60 100 --loop.vis.layerloss.deep_inversion.scale 10 --datamodule.vis.optim.kwargs.lr 0.05 --model.type dla --model.latent.size 10
[34m	Used config: [0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=2, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[True], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=3, schedule=[300, 300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=10.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.001), l2=Namespace(use_at=[False], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=10, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='dla', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=120.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=440, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1, 2], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mWANDBO RUN ID: dqdqvdb8 [0m
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: dla
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_DLA [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ DLA                 â”‚ 32.6 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 32.6 M                                                                                                                                  
Non-trainable params: 0                                                                                                                                   
Total params: 32.6 M                                                                                                                                      
Total estimated model params size (MB): 130                                                                                                               
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] [0m
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 10.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 10.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 10.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mTESTING TASK 0, loop 0 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5511999726295471     â”‚
â”‚      test_loss_epoch      â”‚    -231.34129333496094    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 160/160 0:00:18 â€¢ 0:00:00 9.06it/s   
Testing   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 23/23   0:00:45 â€¢ 0:00:00 24.00it/s  
[34mINFO: hooking model during visualization to - [33mDEEP INVERSION[34m - task: 1, loop 1 [0m
[34mINFO: For layerloss DeepInversionFeatureLoss used with default value: [36m'10.0'[34m  [0m
[34mINFO: hooking model during visualization to - [33mVARIATION IMAGE REGULARIZATION[34m - task: 1, loop 1 [0m
[34mDREAMING DURING TASK: 1, loop 1 [0m
[36mVIS: Visualization for targets: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49} [0m
[34mVIS: No hook to image after forward and backward pass. [0m
[34mVIS: Optimizer set during visualization config: [32mAdam (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0
) [0m
[32mVIS: Image size before (up/down)sample - torch.Size([220, 3, 41, 41]) [0m
[32mVIS: Image size after (up/down)sample - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: ENABLE DREAM TRANSFORMS [0m
[32mVIS: Current in use image size - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: Number of images to visualize (may change in fast-dev-run): 22000; batch size: 220; per target: 440; batches to be generated: 100 [0m
[36mVIS: Using multitarget visualization [0m
[34mTime generating features: 01:43:03 [0m
[34mDREAMING END [0m
[34mSTARTING TASK 1, loop 1 -- classes in task [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 1 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mSelected dream dataloader classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]. Use datasampler=False. Batch size: 220 [0m
[95mINFO: Selected classes for normal dataloader: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mINFO: Use dream dataloader. [0m
[95mINFO: Use dream and normal datasets simultaneously. [0m
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 1, loop 1 [0m
[34mTESTING TASK 1, loop 1 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.28540000319480896    â”‚
â”‚      test_loss_epoch      â”‚    -153.0450439453125     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Iteration: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 500/500 0:01:01 â€¢ 0:00:00 8.18it/s   
                                                                                         
                                                                                         
Epoch 299  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 160/160 0:00:32 â€¢ 0:00:00 5.11it/s   
Testing    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:02 â€¢ 0:00:00 21.83it/s  
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: hooking model during visualization to - [33mDEEP INVERSION[34m - task: 1, loop 2 [0m
[34mINFO: For layerloss DeepInversionFeatureLoss used with default value: [36m'10.0'[34m  [0m
[34mINFO: hooking model during visualization to - [33mVARIATION IMAGE REGULARIZATION[34m - task: 1, loop 2 [0m
[34mDREAMING DURING TASK: 1, loop 2 [0m
[36mVIS: Visualization for targets: {50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99} [0m
[34mVIS: No hook to image after forward and backward pass. [0m
[34mVIS: Optimizer set during visualization config: [32mAdam (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0
) [0m
[32mVIS: Image size before (up/down)sample - torch.Size([220, 3, 41, 41]) [0m
[32mVIS: Image size after (up/down)sample - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: ENABLE DREAM TRANSFORMS [0m
[32mVIS: Current in use image size - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: Number of images to visualize (may change in fast-dev-run): 22000; batch size: 220; per target: 440; batches to be generated: 100 [0m
[36mVIS: Using multitarget visualization [0m
[34mTime generating features: 01:43:34 [0m
[34mDREAMING END [0m
[34mSTARTING TASK 1, loop 2 -- classes in task [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 1 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[34mENDING TASK 1, loop 2 [0m
[34mTESTING TASK 1, loop 2 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.28540000319480896    â”‚
â”‚      test_loss_epoch      â”‚    -153.0450439453125     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Iteration: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 500/500 0:01:01 â€¢ 0:00:00 8.12it/s   
                                                                                         
                                                                                         
Testing    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:01 â€¢ 0:00:00 23.31it/s  
[34mINFO: Generated current run name: d25-04-2024_h12-07-22_dqdqvdb8 [0m
[31mWARNING: At loop 3 selected last epoch per task "0" because list index out of range. [0m
[31mWARNING: At loop 3 selected last epoch per task "0" because list index out of range. [0m

End of training.
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 10 dimensions.
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h12-07-22_dqdqvdb8/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h12-07-22_dqdqvdb8/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h12-07-22_dqdqvdb8/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h12-07-22_dqdqvdb8/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h12-07-22_dqdqvdb8/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h12-07-22_dqdqvdb8/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h12-07-22_dqdqvdb8/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h12-07-22_dqdqvdb8/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h12-07-22_dqdqvdb8/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h12-07-22_dqdqvdb8/plots/mean_dist_matrix_idx100
wandb: - 10.815 MB of 10.815 MB uploaded (0.851 MB deduped)wandb: \ 10.815 MB of 10.815 MB uploaded (0.851 MB deduped)wandb: | 10.815 MB of 10.815 MB uploaded (0.851 MB deduped)wandb: / 10.815 MB of 10.815 MB uploaded (0.851 MB deduped)wandb: - 10.815 MB of 10.815 MB uploaded (0.851 MB deduped)wandb: \ 10.815 MB of 10.815 MB uploaded (0.851 MB deduped)wandb: | 10.815 MB of 10.815 MB uploaded (0.851 MB deduped)wandb: / 10.815 MB of 10.815 MB uploaded (0.851 MB deduped)wandb: - 10.965 MB of 11.240 MB uploaded (0.851 MB deduped)wandb: \ 11.240 MB of 11.240 MB uploaded (0.851 MB deduped)wandb: | 11.240 MB of 11.240 MB uploaded (0.851 MB deduped)wandb: / 11.240 MB of 11.240 MB uploaded (0.851 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 7.6%             
wandb: 
wandb: Run history:
wandb:     dream_loss/run_0/multi_target_0 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‡â–…â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_1 â–†â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–â–†â–„â–‚â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_10 â–ˆâ–…â–ƒâ–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–†â–…â–ƒâ–‚â–â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_11 â–ˆâ–…â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–„â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_12 â–ˆâ–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–…â–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_13 â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_14 â–ˆâ–†â–ƒâ–‚â–‚â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–‚â–â–‚â–‡â–…â–…â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_15 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–â–â–â–â–â–‚â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_16 â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_17 â–ˆâ–†â–ƒâ–‚â–ƒâ–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–‡â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_18 â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:    dream_loss/run_0/multi_target_19 â–ˆâ–…â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–†â–…â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:     dream_loss/run_0/multi_target_2 â–ˆâ–…â–ƒâ–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–‡â–„â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:    dream_loss/run_0/multi_target_20 â–ˆâ–…â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–†â–…â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–‚â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_21 â–ˆâ–†â–ƒâ–‚â–‚â–â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–‡â–†â–ƒâ–‚â–â–‚â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_22 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–ˆâ–…â–‚â–‚â–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_23 â–ˆâ–…â–ƒâ–‚â–ƒâ–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–‡â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_24 â–‡â–„â–‚â–‚â–‚â–â–â–â–â–‚â–â–‚â–‚â–‚â–â–â–â–‚â–â–â–ˆâ–…â–‚â–â–â–â–â–â–‚â–â–‚â–â–â–â–â–‚â–â–â–â–
wandb:    dream_loss/run_0/multi_target_25 â–‡â–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–†â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–‚
wandb:    dream_loss/run_0/multi_target_26 â–ˆâ–‡â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–‡â–ƒâ–‚â–‚â–ƒâ–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_27 â–†â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–†â–„â–‚â–‚â–‚â–â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–
wandb:    dream_loss/run_0/multi_target_28 â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–†â–„â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_29 â–ˆâ–…â–„â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–†â–…â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_3 â–ˆâ–†â–…â–‚â–‚â–‚â–â–â–ƒâ–â–â–â–â–‚â–â–â–â–â–â–â–†â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_30 â–ˆâ–†â–‚â–‚â–‚â–â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–‚â–â–‚â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_31 â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–†â–„â–ƒâ–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_32 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–‡â–…â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–‚â–
wandb:    dream_loss/run_0/multi_target_33 â–ˆâ–…â–…â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_34 â–ˆâ–„â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_35 â–ˆâ–…â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_36 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–‡â–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_37 â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–…â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_38 â–ˆâ–„â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–…â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_39 â–ˆâ–‡â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_4 â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–†â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_40 â–ˆâ–„â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_41 â–ˆâ–…â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–â–‚â–â–‡â–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_42 â–ˆâ–…â–ƒâ–ƒâ–â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–‚â–‚â–â–â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_43 â–ˆâ–„â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_44 â–ˆâ–…â–‚â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–ˆâ–…â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_45 â–ˆâ–†â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–†â–…â–ƒâ–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚
wandb:    dream_loss/run_0/multi_target_46 â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–‡â–…â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_47 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–ˆâ–ˆâ–„â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_48 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚
wandb:    dream_loss/run_0/multi_target_49 â–ˆâ–…â–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_5 â–‡â–„â–‚â–‚â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–…â–ˆâ–‚â–‚â–â–â–‚â–ƒâ–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_6 â–ˆâ–…â–„â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–†â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_7 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–†â–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_8 â–‡â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–†â–„â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_9 â–ˆâ–…â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–†â–„â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_0 â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–ƒâ–â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_1 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_10 â–ˆâ–†â–„â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–ƒâ–â–â–‚â–â–â–‚â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_11 â–ˆâ–†â–„â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_12 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–†â–†â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_13 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_14 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_15 â–ˆâ–‡â–ƒâ–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–‡â–†â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_16 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–
wandb:    dream_loss/run_1/multi_target_17 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–†â–‡â–ƒâ–‚â–‚â–ƒâ–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_18 â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–ƒâ–‚â–â–â–‚â–â–â–â–â–‚â–â–â–â–‚â–
wandb:    dream_loss/run_1/multi_target_19 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–†â–…â–ƒâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_2 â–ˆâ–‡â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–
wandb:    dream_loss/run_1/multi_target_20 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_21 â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–†â–…â–ƒâ–‚â–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_22 â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–‡â–†â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_23 â–ˆâ–‡â–„â–‚â–‚â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_24 â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_25 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_26 â–ˆâ–‡â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–ˆâ–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:    dream_loss/run_1/multi_target_27 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_28 â–ˆâ–‡â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–ˆâ–‡â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_29 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–‡â–‡â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_3 â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_30 â–ˆâ–…â–„â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–‡â–†â–ƒâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_31 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_32 â–ˆâ–‡â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_33 â–ˆâ–†â–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_34 â–ˆâ–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–†â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_35 â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–ˆâ–†â–ƒâ–‚â–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_36 â–ˆâ–†â–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–
wandb:    dream_loss/run_1/multi_target_37 â–ˆâ–†â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–
wandb:    dream_loss/run_1/multi_target_38 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_39 â–ˆâ–†â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–†â–†â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–
wandb:     dream_loss/run_1/multi_target_4 â–ˆâ–…â–…â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–‚â–â–‡â–‡â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_40 â–ˆâ–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–‡â–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_41 â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‡â–…â–„â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_42 â–ˆâ–†â–„â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–„â–‚â–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–‚â–
wandb:    dream_loss/run_1/multi_target_43 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–†â–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_44 â–ˆâ–†â–„â–ƒâ–„â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_45 â–ˆâ–†â–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–‡â–‡â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_46 â–ˆâ–†â–ƒâ–„â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_47 â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_48 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–†â–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–‚
wandb:    dream_loss/run_1/multi_target_49 â–ˆâ–‡â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_5 â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_6 â–ˆâ–‡â–…â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_7 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_8 â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–‚â–â–â–‚
wandb:     dream_loss/run_1/multi_target_9 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–‡â–†â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–
wandb:                               epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆ
wandb:                       negative_loss â–ˆâ–†â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       positive_loss â–ˆâ–ˆâ–†â–†â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              stats/collect_accuracy â–â–â–„â–ˆâ–„â–…â–…â–…â–†â–†â–‡â–‡
wandb:                  stats/collect_loss â–ƒâ–ˆâ–„â–„â–ˆâ–…â–â–„â–‚â–ƒâ–‚â–‚
wandb:                            test_acc â–ˆâ–â–
wandb:                     test_loss_epoch â–â–ˆâ–ˆ
wandb:                      test_loss_step â–‚â–â–‚â–â–ƒâ–‚â–ƒâ–„â–…â–…â–„â–…â–…â–†â–„â–ƒâ–„â–…â–ˆâ–†â–†â–„â–…â–†â–‡â–‡â–…â–…â–…â–†â–„â–‡â–†â–†â–…â–†â–†â–…â–„â–ƒ
wandb:                   train_loss/island â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    train_loss/total â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             train_loss_dream/island â–ˆâ–†â–…â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              train_loss_dream/total â–ˆâ–†â–…â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                      train_step_acc â–â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‚â–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                train_step_acc_dream â–â–‚â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 trainer/global_step â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: val_last_step_loss/dataloader_idx_0 â–ˆâ–†â–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb: val_last_step_loss/dataloader_idx_1 â–ˆâ–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          valid_acc/dataloader_idx_0 â–‚â–‚â–ƒâ–„â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          valid_acc/dataloader_idx_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:     dream_loss/run_0/multi_target_0 9232.85254
wandb:     dream_loss/run_0/multi_target_1 23177.57031
wandb:    dream_loss/run_0/multi_target_10 12141.75195
wandb:    dream_loss/run_0/multi_target_11 11513.17773
wandb:    dream_loss/run_0/multi_target_12 13841.91016
wandb:    dream_loss/run_0/multi_target_13 13468.43066
wandb:    dream_loss/run_0/multi_target_14 9973.47168
wandb:    dream_loss/run_0/multi_target_15 11828.55078
wandb:    dream_loss/run_0/multi_target_16 10333.97949
wandb:    dream_loss/run_0/multi_target_17 11661.53223
wandb:    dream_loss/run_0/multi_target_18 12192.49902
wandb:    dream_loss/run_0/multi_target_19 10258.9668
wandb:     dream_loss/run_0/multi_target_2 12282.80371
wandb:    dream_loss/run_0/multi_target_20 11522.72949
wandb:    dream_loss/run_0/multi_target_21 11589.16699
wandb:    dream_loss/run_0/multi_target_22 9805.0918
wandb:    dream_loss/run_0/multi_target_23 7858.76367
wandb:    dream_loss/run_0/multi_target_24 10767.51855
wandb:    dream_loss/run_0/multi_target_25 11518.0918
wandb:    dream_loss/run_0/multi_target_26 11433.57812
wandb:    dream_loss/run_0/multi_target_27 10607.5752
wandb:    dream_loss/run_0/multi_target_28 10376.10938
wandb:    dream_loss/run_0/multi_target_29 9096.20703
wandb:     dream_loss/run_0/multi_target_3 9324.3252
wandb:    dream_loss/run_0/multi_target_30 10832.3623
wandb:    dream_loss/run_0/multi_target_31 9465.10156
wandb:    dream_loss/run_0/multi_target_32 11317.35938
wandb:    dream_loss/run_0/multi_target_33 12369.48535
wandb:    dream_loss/run_0/multi_target_34 8558.20898
wandb:    dream_loss/run_0/multi_target_35 10377.42285
wandb:    dream_loss/run_0/multi_target_36 10667.36328
wandb:    dream_loss/run_0/multi_target_37 9203.19141
wandb:    dream_loss/run_0/multi_target_38 27015.62695
wandb:    dream_loss/run_0/multi_target_39 9669.27441
wandb:     dream_loss/run_0/multi_target_4 11385.42773
wandb:    dream_loss/run_0/multi_target_40 13845.08398
wandb:    dream_loss/run_0/multi_target_41 28880.83008
wandb:    dream_loss/run_0/multi_target_42 10059.13477
wandb:    dream_loss/run_0/multi_target_43 12483.1084
wandb:    dream_loss/run_0/multi_target_44 12460.28516
wandb:    dream_loss/run_0/multi_target_45 9032.25684
wandb:    dream_loss/run_0/multi_target_46 10693.41309
wandb:    dream_loss/run_0/multi_target_47 10555.38477
wandb:    dream_loss/run_0/multi_target_48 11893.63965
wandb:    dream_loss/run_0/multi_target_49 11697.59277
wandb:     dream_loss/run_0/multi_target_5 10195.80273
wandb:     dream_loss/run_0/multi_target_6 10321.43066
wandb:     dream_loss/run_0/multi_target_7 10344.52344
wandb:     dream_loss/run_0/multi_target_8 13507.63477
wandb:     dream_loss/run_0/multi_target_9 12038.51855
wandb:     dream_loss/run_1/multi_target_0 13766.67871
wandb:     dream_loss/run_1/multi_target_1 17126.79297
wandb:    dream_loss/run_1/multi_target_10 13586.42578
wandb:    dream_loss/run_1/multi_target_11 12990.60156
wandb:    dream_loss/run_1/multi_target_12 15301.75098
wandb:    dream_loss/run_1/multi_target_13 22005.19727
wandb:    dream_loss/run_1/multi_target_14 19858.61914
wandb:    dream_loss/run_1/multi_target_15 16596.98828
wandb:    dream_loss/run_1/multi_target_16 26393.79297
wandb:    dream_loss/run_1/multi_target_17 14882.09082
wandb:    dream_loss/run_1/multi_target_18 22869.03125
wandb:    dream_loss/run_1/multi_target_19 12981.64941
wandb:     dream_loss/run_1/multi_target_2 19475.54492
wandb:    dream_loss/run_1/multi_target_20 18032.94922
wandb:    dream_loss/run_1/multi_target_21 15500.25293
wandb:    dream_loss/run_1/multi_target_22 13286.4248
wandb:    dream_loss/run_1/multi_target_23 14535.52539
wandb:    dream_loss/run_1/multi_target_24 16503.66602
wandb:    dream_loss/run_1/multi_target_25 14100.13281
wandb:    dream_loss/run_1/multi_target_26 12566.39453
wandb:    dream_loss/run_1/multi_target_27 17842.35742
wandb:    dream_loss/run_1/multi_target_28 21185.87695
wandb:    dream_loss/run_1/multi_target_29 14155.25098
wandb:     dream_loss/run_1/multi_target_3 16723.92383
wandb:    dream_loss/run_1/multi_target_30 19300.48047
wandb:    dream_loss/run_1/multi_target_31 11599.61914
wandb:    dream_loss/run_1/multi_target_32 15253.04785
wandb:    dream_loss/run_1/multi_target_33 12086.94238
wandb:    dream_loss/run_1/multi_target_34 18170.38477
wandb:    dream_loss/run_1/multi_target_35 18038.35742
wandb:    dream_loss/run_1/multi_target_36 14257.78906
wandb:    dream_loss/run_1/multi_target_37 31700.69922
wandb:    dream_loss/run_1/multi_target_38 14550.60254
wandb:    dream_loss/run_1/multi_target_39 17234.67578
wandb:     dream_loss/run_1/multi_target_4 14983.02539
wandb:    dream_loss/run_1/multi_target_40 14564.46777
wandb:    dream_loss/run_1/multi_target_41 18772.21289
wandb:    dream_loss/run_1/multi_target_42 15318.90918
wandb:    dream_loss/run_1/multi_target_43 17730.17578
wandb:    dream_loss/run_1/multi_target_44 15952.0
wandb:    dream_loss/run_1/multi_target_45 18876.41016
wandb:    dream_loss/run_1/multi_target_46 13050.7334
wandb:    dream_loss/run_1/multi_target_47 14119.06152
wandb:    dream_loss/run_1/multi_target_48 18929.24805
wandb:    dream_loss/run_1/multi_target_49 15405.89258
wandb:     dream_loss/run_1/multi_target_5 13693.12598
wandb:     dream_loss/run_1/multi_target_6 17069.13086
wandb:     dream_loss/run_1/multi_target_7 17321.29688
wandb:     dream_loss/run_1/multi_target_8 16681.66797
wandb:     dream_loss/run_1/multi_target_9 12679.38379
wandb:                               epoch 0
wandb:                       negative_loss -413.82269
wandb:                       positive_loss -3.79494
wandb:                               ratio 10.0
wandb:                         rho_sigma_2 100.0
wandb:                               scale 120.0
wandb:              stats/collect_accuracy 0.27841
wandb:                  stats/collect_loss -171.45074
wandb:                            test_acc 0.2854
wandb:                     test_loss_epoch -153.04504
wandb:                      test_loss_step -194.28084
wandb:                   train_loss/island -414.54834
wandb:                    train_loss/total -414.54834
wandb:             train_loss_dream/island -417.67313
wandb:              train_loss_dream/total -417.67313
wandb:                      train_step_acc 0.99836
wandb:                train_step_acc_dream 0.9996
wandb:                 trainer/global_step 2420
wandb: val_last_step_loss/dataloader_idx_0 6.87606
wandb: val_last_step_loss/dataloader_idx_1 -212.64944
wandb:          valid_acc/dataloader_idx_0 0.0378
wandb:          valid_acc/dataloader_idx_1 0.533
wandb: 
wandb: ğŸš€ View run exp_search_model_type_dla_latent_size_10_dqdqvdb8_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28/runs/dqdqvdb8
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28
wandb: Synced 6 W&B file(s), 213 media file(s), 1307 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240425_042229-dqdqvdb8/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240425_120844-02zrtcxv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_vgg_latent_size_10_02zrtcxv_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28/runs/02zrtcxv
End of experiment: exp_search_model_type_dla_latent_size_10; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: exp_search_model_type_vgg_latent_size_10; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Full second command line: [0m
-d c100 --model.num_classes 100 --config.num_tasks 2 --loop.schedule 300 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 3 --loop.train_at True --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 2 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at False --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam --datamodule.vis.image_type pixel --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 440 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.001 --model.loss.chi.ratio_milestones 40 60 100 --loop.vis.layerloss.deep_inversion.scale 10 --datamodule.vis.optim.kwargs.lr 0.05 --model.type vgg --model.latent.size 10
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=2, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[True], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=3, schedule=[300, 300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=10.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.001), l2=Namespace(use_at=[False], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=10, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='vgg', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=120.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=440, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1, 2], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mWANDBO RUN ID: 02zrtcxv [0m
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: vgg
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: Generated:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                                                                   
Non-trainable params: 0                                                                                                                                   
Total params: 9.2 M                                                                                                                                       
Total estimated model params size (MB): 36                                                                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 10.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 10.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 10.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mTESTING TASK 0, loop 0 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.6126000285148621     â”‚
â”‚      test_loss_epoch      â”‚    -258.3179931640625     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 160/160 0:00:08 â€¢ 0:00:00 18.72it/s  
Testing   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 23/23   0:00:59 â€¢ 0:00:00 46.76it/s  
[34mINFO: hooking model during visualization to - [33mDEEP INVERSION[34m - task: 1, loop 1 [0m
[34mINFO: For layerloss DeepInversionFeatureLoss used with default value: [36m'10.0'[34m  [0m
[34mINFO: hooking model during visualization to - [33mVARIATION IMAGE REGULARIZATION[34m - task: 1, loop 1 [0m
[34mDREAMING DURING TASK: 1, loop 1 [0m
[36mVIS: Visualization for targets: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49} [0m
[34mVIS: No hook to image after forward and backward pass. [0m
[34mVIS: Optimizer set during visualization config: [32mAdam (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0
) [0m
[32mVIS: Image size before (up/down)sample - torch.Size([220, 3, 41, 41]) [0m
[32mVIS: Image size after (up/down)sample - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: ENABLE DREAM TRANSFORMS [0m
[32mVIS: Current in use image size - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: Number of images to visualize (may change in fast-dev-run): 22000; batch size: 220; per target: 440; batches to be generated: 100 [0m
[36mVIS: Using multitarget visualization [0m
[34mTime generating features: 00:20:32 [0m
[34mDREAMING END [0m
[34mSTARTING TASK 1, loop 1 -- classes in task [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 1 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mSelected dream dataloader classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]. Use datasampler=False. Batch size: 220 [0m
[95mINFO: Selected classes for normal dataloader: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mINFO: Use dream dataloader. [0m
[95mINFO: Use dream and normal datasets simultaneously. [0m
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 1, loop 1 [0m
[34mTESTING TASK 1, loop 1 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.24160000681877136    â”‚
â”‚      test_loss_epoch      â”‚    -103.29049682617188    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Iteration: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 500/500 0:00:11 â€¢ 0:00:00 44.58it/s  
                                                                                         
                                                                                         
Epoch 299  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 160/160 0:00:16 â€¢ 0:00:00 10.32it/s  
Testing    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:01 â€¢ 0:00:00 39.64it/s  
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: hooking model during visualization to - [33mDEEP INVERSION[34m - task: 1, loop 2 [0m
[34mINFO: For layerloss DeepInversionFeatureLoss used with default value: [36m'10.0'[34m  [0m
[34mINFO: hooking model during visualization to - [33mVARIATION IMAGE REGULARIZATION[34m - task: 1, loop 2 [0m
[34mDREAMING DURING TASK: 1, loop 2 [0m
[36mVIS: Visualization for targets: {50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99} [0m
[34mVIS: No hook to image after forward and backward pass. [0m
[34mVIS: Optimizer set during visualization config: [32mAdam (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0
) [0m
[32mVIS: Image size before (up/down)sample - torch.Size([220, 3, 41, 41]) [0m
[32mVIS: Image size after (up/down)sample - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: ENABLE DREAM TRANSFORMS [0m
[32mVIS: Current in use image size - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: Number of images to visualize (may change in fast-dev-run): 22000; batch size: 220; per target: 440; batches to be generated: 100 [0m
[36mVIS: Using multitarget visualization [0m
[34mTime generating features: 00:20:32 [0m
[34mDREAMING END [0m
[34mSTARTING TASK 1, loop 2 -- classes in task [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 1 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[34mENDING TASK 1, loop 2 [0m
[34mTESTING TASK 1, loop 2 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.24160000681877136    â”‚
â”‚      test_loss_epoch      â”‚    -103.29049682617188    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Iteration: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 500/500 0:00:12 â€¢ 0:00:00 40.12it/s  
                                                                                         
                                                                                         
Testing    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:01 â€¢ 0:00:00 41.19it/s  
[34mINFO: Generated current run name: d25-04-2024_h14-59-08_02zrtcxv [0m
[31mWARNING: At loop 3 selected last epoch per task "0" because list index out of range. [0m
[31mWARNING: At loop 3 selected last epoch per task "0" because list index out of range. [0m

End of training.
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 10 dimensions.
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h14-59-08_02zrtcxv/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h14-59-08_02zrtcxv/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h14-59-08_02zrtcxv/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h14-59-08_02zrtcxv/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h14-59-08_02zrtcxv/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h14-59-08_02zrtcxv/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h14-59-08_02zrtcxv/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h14-59-08_02zrtcxv/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h14-59-08_02zrtcxv/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h14-59-08_02zrtcxv/plots/mean_dist_matrix_idx100
wandb: - 11.246 MB of 11.246 MB uploaded (0.986 MB deduped)wandb: \ 11.246 MB of 11.246 MB uploaded (0.986 MB deduped)wandb: | 11.246 MB of 11.246 MB uploaded (0.986 MB deduped)wandb: / 11.395 MB of 11.670 MB uploaded (0.988 MB deduped)wandb: - 11.395 MB of 11.670 MB uploaded (0.988 MB deduped)wandb: \ 11.670 MB of 11.670 MB uploaded (0.988 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 8.5%             
wandb: 
wandb: Run history:
wandb:     dream_loss/run_0/multi_target_0 â–ˆâ–ƒâ–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_1 â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_10 â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_11 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_12 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_13 â–ˆâ–ƒâ–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_14 â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–†â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_15 â–ˆâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–†â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_16 â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_17 â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_18 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_19 â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_2 â–ˆâ–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_20 â–ˆâ–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_21 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_22 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_23 â–ˆâ–ƒâ–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_24 â–ˆâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_25 â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_26 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–…â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_27 â–ˆâ–ƒâ–‚â–ƒâ–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_28 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_29 â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–…â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_3 â–ˆâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:    dream_loss/run_0/multi_target_30 â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–†â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_31 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–†â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_32 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_33 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_34 â–ˆâ–ƒâ–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_35 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_36 â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_37 â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_38 â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_39 â–ˆâ–ƒâ–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_4 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_40 â–ˆâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_41 â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_42 â–ˆâ–ƒâ–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:    dream_loss/run_0/multi_target_43 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_44 â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_45 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_46 â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_47 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_48 â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_49 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_5 â–ˆâ–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_6 â–ˆâ–‚â–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_7 â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_8 â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_9 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_0 â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_1 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_10 â–ˆâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_11 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_12 â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_13 â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_14 â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_15 â–ˆâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–‚â–‚â–‚â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_16 â–ˆâ–„â–‚â–ƒâ–‚â–â–â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_17 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_18 â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–‡â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_19 â–ˆâ–„â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_2 â–ˆâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–‡â–„â–‚â–‚â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_20 â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–†â–„â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_21 â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_22 â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_23 â–ˆâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–
wandb:    dream_loss/run_1/multi_target_24 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–â–
wandb:    dream_loss/run_1/multi_target_25 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_26 â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_27 â–ˆâ–„â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_28 â–ˆâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_29 â–ˆâ–…â–ƒâ–‚â–‚â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_3 â–ˆâ–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_30 â–ˆâ–„â–ƒâ–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_31 â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_32 â–ˆâ–„â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_33 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_34 â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_35 â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_36 â–ˆâ–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_37 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_38 â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_39 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_4 â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_40 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_41 â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_42 â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_43 â–ˆâ–ƒâ–ƒâ–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_44 â–ˆâ–„â–ƒâ–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_45 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_46 â–ˆâ–„â–‚â–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_47 â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_48 â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_49 â–ˆâ–„â–ƒâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–‚â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_5 â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_6 â–ˆâ–„â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_7 â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_8 â–ˆâ–„â–‚â–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_9 â–ˆâ–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                               epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆ
wandb:                       negative_loss â–ˆâ–†â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       positive_loss â–ˆâ–†â–…â–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              stats/collect_accuracy â–‡â–„â–‚â–…â–ƒâ–‚â–â–â–…â–…â–‡â–ˆ
wandb:                  stats/collect_loss â–‚â–ˆâ–„â–„â–†â–…â–‚â–â–â–ƒâ–‚â–
wandb:                            test_acc â–ˆâ–â–
wandb:                     test_loss_epoch â–â–ˆâ–ˆ
wandb:                      test_loss_step â–‚â–â–‚â–â–â–â–‚â–‚â–†â–‡â–†â–†â–‡â–†â–…â–…â–†â–‡â–ˆâ–†â–†â–†â–†â–‡â–ˆâ–‡â–†â–†â–‡â–†â–†â–‡â–‡â–ˆâ–†â–†â–‡â–…â–†â–„
wandb:                   train_loss/island â–ˆâ–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    train_loss/total â–ˆâ–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             train_loss_dream/island â–ˆâ–‡â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              train_loss_dream/total â–ˆâ–‡â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                      train_step_acc â–â–â–â–ƒâ–„â–…â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                train_step_acc_dream â–â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 trainer/global_step â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: val_last_step_loss/dataloader_idx_0 â–ˆâ–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: val_last_step_loss/dataloader_idx_1 â–ˆâ–‡â–‡â–…â–…â–†â–…â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:          valid_acc/dataloader_idx_0 â–‚â–‚â–‚â–„â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          valid_acc/dataloader_idx_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:     dream_loss/run_0/multi_target_0 9835.74414
wandb:     dream_loss/run_0/multi_target_1 15994.07617
wandb:    dream_loss/run_0/multi_target_10 14890.8125
wandb:    dream_loss/run_0/multi_target_11 15234.95898
wandb:    dream_loss/run_0/multi_target_12 10288.27246
wandb:    dream_loss/run_0/multi_target_13 14218.18359
wandb:    dream_loss/run_0/multi_target_14 12918.74414
wandb:    dream_loss/run_0/multi_target_15 8747.40039
wandb:    dream_loss/run_0/multi_target_16 11604.41602
wandb:    dream_loss/run_0/multi_target_17 11368.61426
wandb:    dream_loss/run_0/multi_target_18 12371.75391
wandb:    dream_loss/run_0/multi_target_19 10376.78516
wandb:     dream_loss/run_0/multi_target_2 11650.74805
wandb:    dream_loss/run_0/multi_target_20 11327.38477
wandb:    dream_loss/run_0/multi_target_21 9576.79492
wandb:    dream_loss/run_0/multi_target_22 12568.79395
wandb:    dream_loss/run_0/multi_target_23 8310.2998
wandb:    dream_loss/run_0/multi_target_24 9703.60059
wandb:    dream_loss/run_0/multi_target_25 11199.97266
wandb:    dream_loss/run_0/multi_target_26 9995.0332
wandb:    dream_loss/run_0/multi_target_27 13709.89453
wandb:    dream_loss/run_0/multi_target_28 10725.72754
wandb:    dream_loss/run_0/multi_target_29 9968.61621
wandb:     dream_loss/run_0/multi_target_3 9077.62207
wandb:    dream_loss/run_0/multi_target_30 11092.35449
wandb:    dream_loss/run_0/multi_target_31 11270.54102
wandb:    dream_loss/run_0/multi_target_32 7999.67383
wandb:    dream_loss/run_0/multi_target_33 11284.86523
wandb:    dream_loss/run_0/multi_target_34 9228.74902
wandb:    dream_loss/run_0/multi_target_35 9365.46777
wandb:    dream_loss/run_0/multi_target_36 8374.71484
wandb:    dream_loss/run_0/multi_target_37 11278.93359
wandb:    dream_loss/run_0/multi_target_38 20016.55664
wandb:    dream_loss/run_0/multi_target_39 10673.60547
wandb:     dream_loss/run_0/multi_target_4 14066.50781
wandb:    dream_loss/run_0/multi_target_40 10280.49609
wandb:    dream_loss/run_0/multi_target_41 16242.69141
wandb:    dream_loss/run_0/multi_target_42 9989.32227
wandb:    dream_loss/run_0/multi_target_43 11475.91113
wandb:    dream_loss/run_0/multi_target_44 13095.61523
wandb:    dream_loss/run_0/multi_target_45 11067.52734
wandb:    dream_loss/run_0/multi_target_46 10294.96582
wandb:    dream_loss/run_0/multi_target_47 14738.66602
wandb:    dream_loss/run_0/multi_target_48 11503.8623
wandb:    dream_loss/run_0/multi_target_49 14171.11328
wandb:     dream_loss/run_0/multi_target_5 10651.78516
wandb:     dream_loss/run_0/multi_target_6 10464.9209
wandb:     dream_loss/run_0/multi_target_7 12068.21875
wandb:     dream_loss/run_0/multi_target_8 11071.25
wandb:     dream_loss/run_0/multi_target_9 10367.1748
wandb:     dream_loss/run_1/multi_target_0 12130.84961
wandb:     dream_loss/run_1/multi_target_1 12497.23633
wandb:    dream_loss/run_1/multi_target_10 10921.25977
wandb:    dream_loss/run_1/multi_target_11 10496.15625
wandb:    dream_loss/run_1/multi_target_12 11527.1875
wandb:    dream_loss/run_1/multi_target_13 14966.78613
wandb:    dream_loss/run_1/multi_target_14 16804.75195
wandb:    dream_loss/run_1/multi_target_15 12297.15039
wandb:    dream_loss/run_1/multi_target_16 24908.78125
wandb:    dream_loss/run_1/multi_target_17 11538.97852
wandb:    dream_loss/run_1/multi_target_18 12503.31055
wandb:    dream_loss/run_1/multi_target_19 10794.93555
wandb:     dream_loss/run_1/multi_target_2 11075.25488
wandb:    dream_loss/run_1/multi_target_20 10901.6582
wandb:    dream_loss/run_1/multi_target_21 11596.04199
wandb:    dream_loss/run_1/multi_target_22 18151.60938
wandb:    dream_loss/run_1/multi_target_23 10619.18457
wandb:    dream_loss/run_1/multi_target_24 13476.37891
wandb:    dream_loss/run_1/multi_target_25 9901.6416
wandb:    dream_loss/run_1/multi_target_26 10799.4668
wandb:    dream_loss/run_1/multi_target_27 11999.5459
wandb:    dream_loss/run_1/multi_target_28 14044.06055
wandb:    dream_loss/run_1/multi_target_29 10069.13672
wandb:     dream_loss/run_1/multi_target_3 12635.89941
wandb:    dream_loss/run_1/multi_target_30 12466.83496
wandb:    dream_loss/run_1/multi_target_31 11978.25781
wandb:    dream_loss/run_1/multi_target_32 14231.37012
wandb:    dream_loss/run_1/multi_target_33 12513.32031
wandb:    dream_loss/run_1/multi_target_34 16053.47852
wandb:    dream_loss/run_1/multi_target_35 12853.02051
wandb:    dream_loss/run_1/multi_target_36 11420.46777
wandb:    dream_loss/run_1/multi_target_37 15589.67969
wandb:    dream_loss/run_1/multi_target_38 13236.94238
wandb:    dream_loss/run_1/multi_target_39 14440.19141
wandb:     dream_loss/run_1/multi_target_4 13135.1709
wandb:    dream_loss/run_1/multi_target_40 11067.50781
wandb:    dream_loss/run_1/multi_target_41 14831.07227
wandb:    dream_loss/run_1/multi_target_42 11223.90918
wandb:    dream_loss/run_1/multi_target_43 12209.90527
wandb:    dream_loss/run_1/multi_target_44 11528.48145
wandb:    dream_loss/run_1/multi_target_45 10816.28516
wandb:    dream_loss/run_1/multi_target_46 13747.73438
wandb:    dream_loss/run_1/multi_target_47 11887.06445
wandb:    dream_loss/run_1/multi_target_48 10184.73633
wandb:    dream_loss/run_1/multi_target_49 13011.60449
wandb:     dream_loss/run_1/multi_target_5 11779.28711
wandb:     dream_loss/run_1/multi_target_6 38208.16016
wandb:     dream_loss/run_1/multi_target_7 12951.37109
wandb:     dream_loss/run_1/multi_target_8 15199.50293
wandb:     dream_loss/run_1/multi_target_9 10002.08105
wandb:                               epoch 0
wandb:                       negative_loss -416.92819
wandb:                       positive_loss -3.88507
wandb:                               ratio 10.0
wandb:                         rho_sigma_2 100.0
wandb:                               scale 120.0
wandb:              stats/collect_accuracy 0.23295
wandb:                  stats/collect_loss -119.6275
wandb:                            test_acc 0.2416
wandb:                     test_loss_epoch -103.2905
wandb:                      test_loss_step -182.51018
wandb:                   train_loss/island -416.27298
wandb:                    train_loss/total -416.27298
wandb:             train_loss_dream/island -421.02225
wandb:              train_loss_dream/total -421.02225
wandb:                      train_step_acc 0.99864
wandb:                train_step_acc_dream 1.0
wandb:                 trainer/global_step 2420
wandb: val_last_step_loss/dataloader_idx_0 70.22582
wandb: val_last_step_loss/dataloader_idx_1 -148.16748
wandb:          valid_acc/dataloader_idx_0 0.0334
wandb:          valid_acc/dataloader_idx_1 0.4498
wandb: 
wandb: ğŸš€ View run exp_search_model_type_vgg_latent_size_10_02zrtcxv_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28/runs/02zrtcxv
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28
wandb: Synced 6 W&B file(s), 213 media file(s), 1307 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240425_120844-02zrtcxv/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240425_150017-33ydxe47
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_custom-resnet34_latent_size_10_33ydxe47_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28/runs/33ydxe47
End of experiment: exp_search_model_type_vgg_latent_size_10; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: exp_search_model_type_custom-resnet34_latent_size_10; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Full second command line: [0m
-d c100 --model.num_classes 100 --config.num_tasks 2 --loop.schedule 300 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 3 --loop.train_at True --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 2 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at False --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam --datamodule.vis.image_type pixel --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 440 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.001 --model.loss.chi.ratio_milestones 40 60 100 --loop.vis.layerloss.deep_inversion.scale 10 --datamodule.vis.optim.kwargs.lr 0.05 --model.type custom-resnet34 --model.latent.size 10
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=2, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[True], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=3, schedule=[300, 300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=10.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.001), l2=Namespace(use_at=[False], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=10, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='custom-resnet34', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=120.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=440, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1, 2], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mWANDBO RUN ID: 33ydxe47 [0m
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: custom-resnet34
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: Generated:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_CustomResNet34 [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ CustomResNet34      â”‚ 21.3 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 21.3 M                                                                                                                                  
Non-trainable params: 0                                                                                                                                   
Total params: 21.3 M                                                                                                                                      
Total estimated model params size (MB): 85                                                                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 10.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 10.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 10.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mTESTING TASK 0, loop 0 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.6866000294685364     â”‚
â”‚      test_loss_epoch      â”‚     -274.643310546875     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 160/160 0:00:17 â€¢ 0:00:00 9.11it/s   
Testing   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 23/23   0:00:46 â€¢ 0:00:00 24.37it/s  
wandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.
[34mINFO: hooking model during visualization to - [33mDEEP INVERSION[34m - task: 1, loop 1 [0m
[34mINFO: For layerloss DeepInversionFeatureLoss used with default value: [36m'10.0'[34m  [0m
[34mINFO: hooking model during visualization to - [33mVARIATION IMAGE REGULARIZATION[34m - task: 1, loop 1 [0m
[34mDREAMING DURING TASK: 1, loop 1 [0m
[36mVIS: Visualization for targets: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49} [0m
[34mVIS: No hook to image after forward and backward pass. [0m
[34mVIS: Optimizer set during visualization config: [32mAdam (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0
) [0m
[32mVIS: Image size before (up/down)sample - torch.Size([220, 3, 41, 41]) [0m
[32mVIS: Image size after (up/down)sample - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: ENABLE DREAM TRANSFORMS [0m
[32mVIS: Current in use image size - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: Number of images to visualize (may change in fast-dev-run): 22000; batch size: 220; per target: 440; batches to be generated: 100 [0m
[36mVIS: Using multitarget visualization [0m
[34mTime generating features: 01:38:38 [0m
[34mDREAMING END [0m
[34mSTARTING TASK 1, loop 1 -- classes in task [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 1 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mSelected dream dataloader classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]. Use datasampler=False. Batch size: 220 [0m
[95mINFO: Selected classes for normal dataloader: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mINFO: Use dream dataloader. [0m
[95mINFO: Use dream and normal datasets simultaneously. [0m
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 1, loop 1 [0m
[34mTESTING TASK 1, loop 1 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.3467000126838684     â”‚
â”‚      test_loss_epoch      â”‚    -186.54588317871094    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Iteration: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 500/500 0:00:59 â€¢ 0:00:00 8.35it/s   
                                                                                         
                                                                                         
Epoch 299  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 160/160 0:00:32 â€¢ 0:00:00 5.07it/s   
Testing    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:01 â€¢ 0:00:00 23.66it/s  
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: hooking model during visualization to - [33mDEEP INVERSION[34m - task: 1, loop 2 [0m
[34mINFO: For layerloss DeepInversionFeatureLoss used with default value: [36m'10.0'[34m  [0m
[34mINFO: hooking model during visualization to - [33mVARIATION IMAGE REGULARIZATION[34m - task: 1, loop 2 [0m
[34mDREAMING DURING TASK: 1, loop 2 [0m
[36mVIS: Visualization for targets: {50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99} [0m
[34mVIS: No hook to image after forward and backward pass. [0m
[34mVIS: Optimizer set during visualization config: [32mAdam (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0
) [0m
[32mVIS: Image size before (up/down)sample - torch.Size([220, 3, 41, 41]) [0m
[32mVIS: Image size after (up/down)sample - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: ENABLE DREAM TRANSFORMS [0m
[32mVIS: Current in use image size - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: Number of images to visualize (may change in fast-dev-run): 22000; batch size: 220; per target: 440; batches to be generated: 100 [0m
[36mVIS: Using multitarget visualization [0m
[34mTime generating features: 01:38:19 [0m
[34mDREAMING END [0m
[34mSTARTING TASK 1, loop 2 -- classes in task [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 1 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[34mENDING TASK 1, loop 2 [0m
[34mTESTING TASK 1, loop 2 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.3467000126838684     â”‚
â”‚      test_loss_epoch      â”‚    -186.54588317871094    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Iteration: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 500/500 0:00:58 â€¢ 0:00:00 8.60it/s   
                                                                                         
                                                                                         
Testing    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:01 â€¢ 0:00:00 24.10it/s  
[34mINFO: Generated current run name: d25-04-2024_h22-33-14_33ydxe47 [0m
[31mWARNING: At loop 3 selected last epoch per task "0" because list index out of range. [0m
[31mWARNING: At loop 3 selected last epoch per task "0" because list index out of range. [0m

End of training.
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 10 dimensions.
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h22-33-14_33ydxe47/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h22-33-14_33ydxe47/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h22-33-14_33ydxe47/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h22-33-14_33ydxe47/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h22-33-14_33ydxe47/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h22-33-14_33ydxe47/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h22-33-14_33ydxe47/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h22-33-14_33ydxe47/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h22-33-14_33ydxe47/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/model_optim_type-sgd/-deep_inversion/image_reg_var/d25-04-2024_h22-33-14_33ydxe47/plots/mean_dist_matrix_idx100
wandb: - 10.869 MB of 10.869 MB uploaded (0.885 MB deduped)wandb: \ 10.869 MB of 10.869 MB uploaded (0.885 MB deduped)wandb: | 10.869 MB of 10.869 MB uploaded (0.885 MB deduped)wandb: / 11.019 MB of 11.295 MB uploaded (0.887 MB deduped)wandb: - 11.295 MB of 11.295 MB uploaded (0.887 MB deduped)wandb: \ 11.295 MB of 11.295 MB uploaded (0.887 MB deduped)wandb: | 11.295 MB of 11.295 MB uploaded (0.887 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 7.9%             
wandb: 
wandb: Run history:
wandb:     dream_loss/run_0/multi_target_0 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_1 â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_10 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_11 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_12 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_13 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–„â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_14 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_15 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_16 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_17 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_18 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_19 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_2 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_20 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_21 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_22 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_23 â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_24 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_25 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_26 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_27 â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_28 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_29 â–ˆâ–…â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_3 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_30 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_31 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_32 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_33 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_34 â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_35 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_36 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_37 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_38 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_39 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_4 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_40 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_41 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_42 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_43 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_44 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_45 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_46 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_47 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_48 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_0/multi_target_49 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_5 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_6 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_7 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_8 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_0/multi_target_9 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_0 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_1 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_10 â–ˆâ–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_11 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_12 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_13 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_14 â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_15 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_16 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_17 â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_18 â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_19 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–‡â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_2 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–
wandb:    dream_loss/run_1/multi_target_20 â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_21 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_22 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_23 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_24 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_25 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_26 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_27 â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_28 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_29 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_3 â–ˆâ–„â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_30 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_31 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_32 â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_33 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_34 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_35 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_36 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_37 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_38 â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_39 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_4 â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_40 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_41 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_42 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_43 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_44 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_45 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_46 â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_47 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_48 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    dream_loss/run_1/multi_target_49 â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_5 â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_6 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_7 â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_8 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:     dream_loss/run_1/multi_target_9 â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–
wandb:                               epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆ
wandb:                       negative_loss â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       positive_loss â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              stats/collect_accuracy â–ˆâ–…â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚
wandb:                  stats/collect_loss â–â–ˆâ–‡â–‡â–†â–†â–‡â–‚â–‚â–†â–‚â–‡
wandb:                            test_acc â–ˆâ–â–
wandb:                     test_loss_epoch â–â–ˆâ–ˆ
wandb:                      test_loss_step â–‚â–ƒâ–â–â–ƒâ–ƒâ–ƒâ–‚â–…â–‡â–‡â–†â–‡â–ˆâ–…â–†â–…â–…â–‡â–‡â–†â–…â–‡â–†â–‡â–†â–…â–†â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–…â–ˆâ–‡â–‡â–…â–„
wandb:                   train_loss/island â–ˆâ–†â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    train_loss/total â–ˆâ–†â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             train_loss_dream/island â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              train_loss_dream/total â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                      train_step_acc â–â–â–‚â–ƒâ–„â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                train_step_acc_dream â–â–„â–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 trainer/global_step â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: val_last_step_loss/dataloader_idx_0 â–ˆâ–‡â–„â–ƒâ–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–„â–…â–†â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb: val_last_step_loss/dataloader_idx_1 â–ˆâ–ˆâ–‡â–†â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          valid_acc/dataloader_idx_0 â–‚â–â–ƒâ–„â–„â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          valid_acc/dataloader_idx_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:     dream_loss/run_0/multi_target_0 38207.71875
wandb:     dream_loss/run_0/multi_target_1 56535.36719
wandb:    dream_loss/run_0/multi_target_10 38425.77344
wandb:    dream_loss/run_0/multi_target_11 49923.46484
wandb:    dream_loss/run_0/multi_target_12 57719.59375
wandb:    dream_loss/run_0/multi_target_13 46766.24219
wandb:    dream_loss/run_0/multi_target_14 44142.75781
wandb:    dream_loss/run_0/multi_target_15 41010.53516
wandb:    dream_loss/run_0/multi_target_16 38627.27734
wandb:    dream_loss/run_0/multi_target_17 47153.11328
wandb:    dream_loss/run_0/multi_target_18 27438.60742
wandb:    dream_loss/run_0/multi_target_19 42633.1875
wandb:     dream_loss/run_0/multi_target_2 41403.28906
wandb:    dream_loss/run_0/multi_target_20 35271.35938
wandb:    dream_loss/run_0/multi_target_21 44590.15234
wandb:    dream_loss/run_0/multi_target_22 45192.23438
wandb:    dream_loss/run_0/multi_target_23 28311.08594
wandb:    dream_loss/run_0/multi_target_24 42242.98828
wandb:    dream_loss/run_0/multi_target_25 38491.25781
wandb:    dream_loss/run_0/multi_target_26 38256.53906
wandb:    dream_loss/run_0/multi_target_27 47052.88672
wandb:    dream_loss/run_0/multi_target_28 47947.03906
wandb:    dream_loss/run_0/multi_target_29 43298.75
wandb:     dream_loss/run_0/multi_target_3 32096.80664
wandb:    dream_loss/run_0/multi_target_30 41943.60156
wandb:    dream_loss/run_0/multi_target_31 43403.92578
wandb:    dream_loss/run_0/multi_target_32 39877.67969
wandb:    dream_loss/run_0/multi_target_33 39433.14062
wandb:    dream_loss/run_0/multi_target_34 37119.61719
wandb:    dream_loss/run_0/multi_target_35 38563.34375
wandb:    dream_loss/run_0/multi_target_36 50175.66016
wandb:    dream_loss/run_0/multi_target_37 35252.61719
wandb:    dream_loss/run_0/multi_target_38 54546.34766
wandb:    dream_loss/run_0/multi_target_39 46872.125
wandb:     dream_loss/run_0/multi_target_4 51924.91016
wandb:    dream_loss/run_0/multi_target_40 41029.65625
wandb:    dream_loss/run_0/multi_target_41 51105.20703
wandb:    dream_loss/run_0/multi_target_42 49413.08203
wandb:    dream_loss/run_0/multi_target_43 41133.37109
wandb:    dream_loss/run_0/multi_target_44 43340.71484
wandb:    dream_loss/run_0/multi_target_45 39636.16797
wandb:    dream_loss/run_0/multi_target_46 31267.56445
wandb:    dream_loss/run_0/multi_target_47 40036.41016
wandb:    dream_loss/run_0/multi_target_48 33753.73438
wandb:    dream_loss/run_0/multi_target_49 35039.90234
wandb:     dream_loss/run_0/multi_target_5 37733.66406
wandb:     dream_loss/run_0/multi_target_6 33647.80078
wandb:     dream_loss/run_0/multi_target_7 44808.29688
wandb:     dream_loss/run_0/multi_target_8 36972.66797
wandb:     dream_loss/run_0/multi_target_9 41248.02344
wandb:     dream_loss/run_1/multi_target_0 81947.32031
wandb:     dream_loss/run_1/multi_target_1 61350.76562
wandb:    dream_loss/run_1/multi_target_10 61395.03906
wandb:    dream_loss/run_1/multi_target_11 62500.53906
wandb:    dream_loss/run_1/multi_target_12 74244.74219
wandb:    dream_loss/run_1/multi_target_13 87357.5
wandb:    dream_loss/run_1/multi_target_14 75093.00781
wandb:    dream_loss/run_1/multi_target_15 74745.99219
wandb:    dream_loss/run_1/multi_target_16 81463.64844
wandb:    dream_loss/run_1/multi_target_17 57649.25781
wandb:    dream_loss/run_1/multi_target_18 58437.05859
wandb:    dream_loss/run_1/multi_target_19 78795.95312
wandb:     dream_loss/run_1/multi_target_2 82700.39844
wandb:    dream_loss/run_1/multi_target_20 64220.05078
wandb:    dream_loss/run_1/multi_target_21 66666.27344
wandb:    dream_loss/run_1/multi_target_22 61231.98438
wandb:    dream_loss/run_1/multi_target_23 81338.86719
wandb:    dream_loss/run_1/multi_target_24 68723.47656
wandb:    dream_loss/run_1/multi_target_25 67382.92188
wandb:    dream_loss/run_1/multi_target_26 65518.70703
wandb:    dream_loss/run_1/multi_target_27 64020.64062
wandb:    dream_loss/run_1/multi_target_28 68955.125
wandb:    dream_loss/run_1/multi_target_29 64590.61719
wandb:     dream_loss/run_1/multi_target_3 71915.75781
wandb:    dream_loss/run_1/multi_target_30 57510.70312
wandb:    dream_loss/run_1/multi_target_31 60381.57812
wandb:    dream_loss/run_1/multi_target_32 70077.55469
wandb:    dream_loss/run_1/multi_target_33 58812.18359
wandb:    dream_loss/run_1/multi_target_34 80781.00781
wandb:    dream_loss/run_1/multi_target_35 79666.875
wandb:    dream_loss/run_1/multi_target_36 80822.75
wandb:    dream_loss/run_1/multi_target_37 81295.30469
wandb:    dream_loss/run_1/multi_target_38 59965.46875
wandb:    dream_loss/run_1/multi_target_39 78985.08594
wandb:     dream_loss/run_1/multi_target_4 85565.5625
wandb:    dream_loss/run_1/multi_target_40 71377.64062
wandb:    dream_loss/run_1/multi_target_41 70377.21875
wandb:    dream_loss/run_1/multi_target_42 65712.08594
wandb:    dream_loss/run_1/multi_target_43 64571.15625
wandb:    dream_loss/run_1/multi_target_44 60760.82031
wandb:    dream_loss/run_1/multi_target_45 73713.09375
wandb:    dream_loss/run_1/multi_target_46 82133.94531
wandb:    dream_loss/run_1/multi_target_47 88160.3125
wandb:    dream_loss/run_1/multi_target_48 66927.94531
wandb:    dream_loss/run_1/multi_target_49 65317.82031
wandb:     dream_loss/run_1/multi_target_5 57547.17188
wandb:     dream_loss/run_1/multi_target_6 99219.45312
wandb:     dream_loss/run_1/multi_target_7 66658.86719
wandb:     dream_loss/run_1/multi_target_8 87015.72656
wandb:     dream_loss/run_1/multi_target_9 76567.34375
wandb:                               epoch 0
wandb:                       negative_loss -414.31699
wandb:                       positive_loss -3.8051
wandb:                               ratio 10.0
wandb:                         rho_sigma_2 100.0
wandb:                               scale 120.0
wandb:              stats/collect_accuracy 0.35265
wandb:                  stats/collect_loss -173.83583
wandb:                            test_acc 0.3467
wandb:                     test_loss_epoch -186.54588
wandb:                      test_loss_step -228.62259
wandb:                   train_loss/island -417.70779
wandb:                    train_loss/total -417.70779
wandb:             train_loss_dream/island -418.1658
wandb:              train_loss_dream/total -418.1658
wandb:                      train_step_acc 0.99184
wandb:                train_step_acc_dream 0.99406
wandb:                 trainer/global_step 2420
wandb: val_last_step_loss/dataloader_idx_0 -37.16906
wandb: val_last_step_loss/dataloader_idx_1 -261.14658
wandb:          valid_acc/dataloader_idx_0 0.0872
wandb:          valid_acc/dataloader_idx_1 0.6062
wandb: 
wandb: ğŸš€ View run exp_search_model_type_custom-resnet34_latent_size_10_33ydxe47_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28/runs/33ydxe47
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-25%3D04-22-28
wandb: Synced 6 W&B file(s), 213 media file(s), 1307 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240425_150017-33ydxe47/logs
End of experiment: exp_search_model_type_custom-resnet34_latent_size_10; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
