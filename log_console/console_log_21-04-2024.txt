Global seed set to 2024
wandb: Currently logged in as: 01133344 (cccvb). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in log_run/wandb/run-20240421_060543-16dq3vgz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_220_16dq3vgz_latent-multitarget_dull_
wandb: ⭐️ View project at https://wandb.ai/cccvb/exp_2024-04-12-11-10-23_DLA
wandb: 🚀 View run at https://wandb.ai/cccvb/exp_2024-04-12-11-10-23_DLA/runs/16dq3vgz
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Experiments to be run:
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type DLA --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type DLA --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 320

Running experiment: exp_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_220; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-12-11-10-23_DLA
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=True, save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='DLA', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=160.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, enable_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-12-11-10-23_DLA']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: DLA
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_DLA [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                 ┃ Type                ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ train_acc            │ MulticlassAccuracy  │      0 │
│ 1 │ train_acc_dream      │ MulticlassAccuracy  │      0 │
│ 2 │ _valid_accs          │ ModuleDict          │      0 │
│ 3 │ test_acc             │ MulticlassAccuracy  │      0 │
│ 4 │ model                │ DLA                 │ 32.6 M │
│ 5 │ cyclic_latent_buffer │ CyclicBufferByClass │      0 │
│ 6 │ _loss_f              │ ChiLoss             │      0 │
└───┴──────────────────────┴─────────────────────┴────────┘
Trainable params: 32.6 M                                                                                                                               
Non-trainable params: 0                                                                                                                                
Total params: 32.6 M                                                                                                                                   
Total estimated model params size (MB): 130                                                                                                            
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
wandb: Network error (ReadTimeout), entering retry loop.
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mTESTING TASK 0, loop 0 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_acc          │    0.6227999925613403     │
│      test_loss_epoch      │       -717917.9375        │
└───────────────────────────┴───────────────────────────┘
Epoch 299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 274/274 0:01:10 • 0:00:00 4.46it/s   
Testing   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46/46   0:00:54 • 0:00:00 10.89it/s  
[34mINFO: Generated current run name: d21-04-2024_h12-01-23_16dq3vgz [0m
[31mWARNING: At loop 1 selected last epoch per task "300" because list index out of range. [0m

End of training.
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
/home/ubuntu/continual_dreaming/stats/point_plot.py:369: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  fig.savefig(n)
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/wandb/sdk/data_types/image.py:302: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  util.ensure_matplotlib_figure(data).savefig(buf, format="png")
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 30 dimensions.
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h12-01-23_16dq3vgz/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h12-01-23_16dq3vgz/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h12-01-23_16dq3vgz/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h12-01-23_16dq3vgz/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h12-01-23_16dq3vgz/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h12-01-23_16dq3vgz/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h12-01-23_16dq3vgz/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h12-01-23_16dq3vgz/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h12-01-23_16dq3vgz/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h12-01-23_16dq3vgz/plots/mean_dist_matrix_idx100
wandb: - 6.615 MB of 6.615 MB uploadedwandb: \ 6.615 MB of 6.615 MB uploadedwandb: | 6.615 MB of 6.615 MB uploadedwandb: / 6.754 MB of 6.995 MB uploaded (0.002 MB deduped)wandb: - 6.754 MB of 6.995 MB uploaded (0.002 MB deduped)wandb: \ 6.754 MB of 6.995 MB uploaded (0.002 MB deduped)wandb: | 6.995 MB of 6.995 MB uploaded (0.002 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:          negative_loss ███████████▆▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          positive_loss ▁▁▁▁▁▁▁▁▂▁▁▃▂█▆▅▅▅▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂
wandb:                  ratio ▁▁▁▁▁▁▁▁▂▂▂▄▄███████████████████████████
wandb:            rho_sigma_2 ▁▁▁▁▁▁▁▁▁▁▁▃▃███████████████████████████
wandb:                  scale ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: stats/collect_accuracy ██▄▄▁▁▁▁▂▂▂▂
wandb:     stats/collect_loss ▁▄█▅▆▁▁▅▂▂▆▃
wandb:               test_acc ▁
wandb:        test_loss_epoch ▁
wandb:         test_loss_step ▃▄▆▄▅▂▂▃▃▅▃▆▃▄▅▂▂█▅█▂▄▂▃▆▆▅▅▃▅▃▆▆▂▁▄▆▄▃▅
wandb:      train_loss/island ███████████▇▇▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss/total ███████████▇▇▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_step_acc ▁▁▁▁▂▂▂▂▂▃▃▃▄▅▄▅▆▆▇▇▇▇██████████████████
wandb:    trainer/global_step ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████▁▁
wandb:     val_last_step_loss ███████████▇▆▆▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:              valid_acc ▁▁▁▂▁▂▂▂▂▃▂▃▄▅▅▅▄▆▇█████████████████████
wandb: 
wandb: Run summary:
wandb:                  epoch 300
wandb:          negative_loss -807268.8125
wandb:          positive_loss 11678.62988
wandb:                  ratio 160.0
wandb:            rho_sigma_2 25600.0
wandb:                  scale 160.0
wandb: stats/collect_accuracy 0.63068
wandb:     stats/collect_loss -723390.0625
wandb:               test_acc 0.6228
wandb:        test_loss_epoch -717917.9375
wandb:         test_loss_step -699113.0625
wandb:      train_loss/island -777173.6875
wandb:       train_loss/total -777173.6875
wandb:         train_step_acc 0.85694
wandb:    trainer/global_step 2420
wandb:     val_last_step_loss -717917.9375
wandb:              valid_acc 0.6228
wandb: 
wandb: 🚀 View run exp_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_220_16dq3vgz_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-12-11-10-23_DLA/runs/16dq3vgz
wandb: ️⚡ View job at https://wandb.ai/cccvb/exp_2024-04-12-11-10-23_DLA/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2MjA0MzExNg==/version_details/v32
wandb: Synced 6 W&B file(s), 113 media file(s), 106 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240421_060543-16dq3vgz/logs
Global seed set to 2024
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in log_run/wandb/run-20240421_120320-dudd703c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_320_dudd703c_latent-multitarget_dull_
wandb: ⭐️ View project at https://wandb.ai/cccvb/exp_2024-04-12-11-10-23_DLA
wandb: 🚀 View run at https://wandb.ai/cccvb/exp_2024-04-12-11-10-23_DLA/runs/dudd703c
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: exp_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_220; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: exp_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_320; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-12-11-10-23_DLA
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=True, save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='DLA', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=160.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, enable_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=320)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-12-11-10-23_DLA']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: DLA
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_DLA [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                 ┃ Type                ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ train_acc            │ MulticlassAccuracy  │      0 │
│ 1 │ train_acc_dream      │ MulticlassAccuracy  │      0 │
│ 2 │ _valid_accs          │ ModuleDict          │      0 │
│ 3 │ test_acc             │ MulticlassAccuracy  │      0 │
│ 4 │ model                │ DLA                 │ 32.6 M │
│ 5 │ cyclic_latent_buffer │ CyclicBufferByClass │      0 │
│ 6 │ _loss_f              │ ChiLoss             │      0 │
└───┴──────────────────────┴─────────────────────┴────────┘
Trainable params: 32.6 M                                                                                                                               
Non-trainable params: 0                                                                                                                                
Total params: 32.6 M                                                                                                                                   
Total estimated model params size (MB): 130                                                                                                            
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mTESTING TASK 0, loop 0 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_acc          │    0.6302000284194946     │
│      test_loss_epoch      │        -720921.875        │
└───────────────────────────┴───────────────────────────┘
Epoch 299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189/189 0:01:02 • 0:00:00 3.40it/s  
Testing   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32/32   0:00:51 • 0:00:00 9.03it/s  
[34mINFO: Generated current run name: d21-04-2024_h17-22-52_dudd703c [0m
[31mWARNING: At loop 1 selected last epoch per task "300" because list index out of range. [0m

End of training.
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
/home/ubuntu/continual_dreaming/stats/point_plot.py:369: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  fig.savefig(n)
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/wandb/sdk/data_types/image.py:302: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  util.ensure_matplotlib_figure(data).savefig(buf, format="png")
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 30 dimensions.
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h17-22-52_dudd703c/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h17-22-52_dudd703c/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h17-22-52_dudd703c/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h17-22-52_dudd703c/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h17-22-52_dudd703c/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h17-22-52_dudd703c/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h17-22-52_dudd703c/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h17-22-52_dudd703c/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h17-22-52_dudd703c/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/model_optim_type-sgd/d21-04-2024_h17-22-52_dudd703c/plots/mean_dist_matrix_idx100
wandb: - 6.634 MB of 6.634 MB uploadedwandb: \ 6.634 MB of 6.634 MB uploadedwandb: | 6.634 MB of 6.634 MB uploadedwandb: / 6.772 MB of 7.013 MB uploaded (0.004 MB deduped)wandb: - 6.772 MB of 7.013 MB uploaded (0.004 MB deduped)wandb: \ 7.013 MB of 7.013 MB uploaded (0.004 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:          negative_loss ███████████▆▆▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          positive_loss ▁▁▁▁▁▁▁▁▁▁▁▂▂▂█▅▄▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:                  ratio ▁▁▁▁▁▁▁▁▂▂▂▄▄▄██████████████████████████
wandb:            rho_sigma_2 ▁▁▁▁▁▁▁▁▁▁▁▃▃▃██████████████████████████
wandb:                  scale ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: stats/collect_accuracy █▁▁▃▂▂▃▃
wandb:     stats/collect_loss ▂█▇▁▇▃▅▃
wandb:               test_acc ▁
wandb:        test_loss_epoch ▁
wandb:         test_loss_step ▂▄▄▂▄▂▃▂▃▄▃▂▂▄▄▄▂▂▁▃▂▃▂▃▃▃▂▁▄▃▃█
wandb:      train_loss/island ███████████▇▆▆▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss/total ███████████▇▆▆▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_step_acc ▁▁▁▂▂▂▂▃▃▄▄▄▅▅▄▅▅▆▆▇▇▇██████████████████
wandb:    trainer/global_step ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████▁▁
wandb:     val_last_step_loss ███████████▇▇▆▂▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:              valid_acc ▁▁▁▁▂▂▂▃▁▃▃▃▃▅▃▃▃▄▆█████████████████████
wandb: 
wandb: Run summary:
wandb:                  epoch 300
wandb:          negative_loss -805497.9375
wandb:          positive_loss 14218.96484
wandb:                  ratio 160.0
wandb:            rho_sigma_2 25600.0
wandb:                  scale 160.0
wandb: stats/collect_accuracy 0.63359
wandb:     stats/collect_loss -727050.6875
wandb:               test_acc 0.6302
wandb:        test_loss_epoch -720921.875
wandb:         test_loss_step -673651.875
wandb:      train_loss/island -783232.8125
wandb:       train_loss/total -783232.8125
wandb:         train_step_acc 0.83562
wandb:    trainer/global_step 2240
wandb:     val_last_step_loss -720921.875
wandb:              valid_acc 0.6302
wandb: 
wandb: 🚀 View run exp_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_320_dudd703c_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-12-11-10-23_DLA/runs/dudd703c
wandb: ️⚡ View job at https://wandb.ai/cccvb/exp_2024-04-12-11-10-23_DLA/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2MjA0MzExNg==/version_details/v33
wandb: Synced 6 W&B file(s), 113 media file(s), 106 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240421_120320-dudd703c/logs
End of experiment: exp_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_320; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
