Experiments to be run:
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type dla --model.latent.size 3
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type dla --model.latent.size 10
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type dla --model.latent.size 20
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type dla --model.latent.size 30
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type vgg --model.latent.size 3
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type vgg --model.latent.size 10
Global seed set to 2024
wandb: Currently logged in as: 01133344 (cccvb). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240422_180703-taq3ahfx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_dla_latent_size_3_taq3ahfx_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/taq3ahfx
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type vgg --model.latent.size 20
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type vgg --model.latent.size 30
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type custom-resnet34 --model.latent.size 3
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type custom-resnet34 --model.latent.size 10
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type custom-resnet34 --model.latent.size 20
* 
-d c100 --model.num_classes 100 --config.num_tasks 1 --loop.schedule 300 0 --config.framework_type latent-multitarget-multitask --loop.num_loops 2 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable  --model.loss.chi.shift_min_distance 0 --config.seed 2024 --model.loss.chi.ratio 10 --model.loss.chi.scale 20 --datamodule.batch_size 220 --datamodule.vis.only_vis_at False --datamodule.vis.enable_vis_at 1 --loop.vis.image_reg.var.use_at True --loop.vis.image_reg.l2.use_at True --loop.test_at True --loop.vis.layerloss.deep_inversion.use_at True --datamodule.vis.optim.type adam  --datamodule.vis.image_type pixel --datamodule.num_workers 3 --datamodule.vis.threshold 500 --loop.save.dreams --datamodule.vis.multitarget.enable --datamodule.vis.batch_size 220 --datamodule.vis.per_target 220 --loop.vis.generate_at 1 2 --datamodule.vis.standard_image_size 32 --loop.vis.image_reg.var.scale 0.01 --loop.vis.image_reg.l2.coeff 1e-5 --loop.vis.layerloss.deep_inversion.scale 0.1 --datamodule.vis.optim.kwargs.lr 0.05
 --model.type custom-resnet34 --model.latent.size 30

Running experiment: exp_search_model_type_dla_latent_size_3; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=3, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='dla', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: dla
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_DLA [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ DLA                 â”‚ 32.6 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 32.6 M                                                                                                               
Non-trainable params: 0                                                                                                                
Total params: 32.6 M                                                                                                                   
Total estimated model params size (MB): 130                                                                                            
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mTESTING TASK 0, loop 0 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5351999998092651     â”‚
â”‚      test_loss_epoch      â”‚     72.94352722167969     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 274/274 0:00:34 â€¢ 0:00:00 8.10it/s   
Testing   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:45 â€¢ 0:00:00 22.84it/s  
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: hooking model during visualization to - [33mDEEP INVERSION[34m - task: 0, loop 1 [0m
[34mINFO: For layerloss DeepInversionFeatureLoss used with default value: [36m'0.1'[34m  [0m
[34mINFO: hooking model during visualization to - [33mVARIATION IMAGE REGULARIZATION[34m - task: 0, loop 1 [0m
[34mINFO: hooking model during visualization to - [33mL2 IMAGE REGULARIZATION[34m - task: 0, loop 1 [0m
[34mDREAMING DURING TASK: 0, loop 1 [0m
[36mVIS: Visualization for targets: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99} [0m
[34mVIS: No hook to image after forward and backward pass. [0m
[34mVIS: Optimizer set during visualization config: [32mAdam (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0
) [0m
[32mVIS: Image size before (up/down)sample - torch.Size([220, 3, 41, 41]) [0m
[32mVIS: Image size after (up/down)sample - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: ENABLE DREAM TRANSFORMS [0m
[32mVIS: Current in use image size - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: Number of images to visualize (may change in fast-dev-run): 22000; batch size: 220; per target: 220; batches to be generated: 100 [0m
[36mVIS: Using multitarget visualization [0m
[34mTime generating features: 01:42:55 [0m
[34mDREAMING END [0m
[34mSTARTING TASK 0, loop 1 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[31mINFO: SKIPPING ANY TRAINING at loop 1 [0m
[34mENDING TASK 0, loop 1 [0m
[34mTESTING TASK 0, loop 1 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5351999998092651     â”‚
â”‚      test_loss_epoch      â”‚     72.94352722167969     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Iteration: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 500/500 0:01:00 â€¢ 0:00:00 8.20it/s   
                                                                                         
                                                                                         
Testing    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:01 â€¢ 0:00:00 23.53it/s  
[34mINFO: Generated current run name: d22-04-2024_h22-46-25_taq3ahfx [0m
[31mWARNING: At loop 2 selected last epoch per task "0" because list index out of range. [0m
[31mWARNING: At loop 2 selected last epoch per task "0" because list index out of range. [0m

End of training.
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d22-04-2024_h22-46-25_taq3ahfx/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d22-04-2024_h22-46-25_taq3ahfx/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d22-04-2024_h22-46-25_taq3ahfx/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d22-04-2024_h22-46-25_taq3ahfx/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d22-04-2024_h22-46-25_taq3ahfx/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d22-04-2024_h22-46-25_taq3ahfx/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d22-04-2024_h22-46-25_taq3ahfx/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d22-04-2024_h22-46-25_taq3ahfx/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d22-04-2024_h22-46-25_taq3ahfx/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d22-04-2024_h22-46-25_taq3ahfx/plots/mean_dist_matrix_idx100
wandb: - 13.966 MB of 13.966 MB uploaded (2.938 MB deduped)wandb: \ 13.966 MB of 13.966 MB uploaded (2.938 MB deduped)wandb: | 14.116 MB of 14.386 MB uploaded (2.938 MB deduped)wandb: / 14.216 MB of 14.386 MB uploaded (2.938 MB deduped)wandb: - 14.216 MB of 14.386 MB uploaded (2.938 MB deduped)wandb: \ 14.216 MB of 14.386 MB uploaded (2.938 MB deduped)wandb: | 14.216 MB of 14.386 MB uploaded (2.938 MB deduped)wandb: / 14.386 MB of 14.386 MB uploaded (2.938 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 20.4%             
wandb: 
wandb: Run history:
wandb:  dream_loss/run_0/multi_target_0 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_1 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_10 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_11 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_12 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_13 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_14 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_15 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_16 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_17 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_18 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_19 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_2 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_20 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_21 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_22 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_23 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_24 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_25 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_26 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_27 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_28 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_29 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_3 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_30 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_31 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_32 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_33 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_34 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_35 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_36 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_37 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_38 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_39 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_4 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_40 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_41 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_42 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_43 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_44 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_45 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_46 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_47 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_48 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_49 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_5 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_50 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_51 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_52 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_53 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_54 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_55 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_56 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_57 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_58 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_59 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_6 â–ˆâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_60 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_61 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_62 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_63 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_64 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_65 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_66 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_67 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_68 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_69 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_7 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_70 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_71 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_72 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_73 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_74 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_75 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_76 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_77 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_78 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_79 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_8 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_80 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_81 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_82 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_83 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_84 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_85 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_86 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_87 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_88 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_89 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_9 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_90 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_91 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_92 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_93 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_94 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_95 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_96 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_97 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_98 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_99 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    negative_loss â–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    positive_loss â–ˆâ–†â–†â–„â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                      rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           stats/collect_accuracy â–ˆâ–„â–ƒâ–„â–‚â–â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒ
wandb:               stats/collect_loss â–â–ƒâ–ˆâ–†â–†â–ƒâ–‚â–ˆâ–…â–„â–„â–ƒ
wandb:                         test_acc â–â–
wandb:                  test_loss_epoch â–â–
wandb:                   test_loss_step â–‚â–…â–„â–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–ˆâ–‚â–â–„â–„â–ƒâ–…â–ƒâ–‚â–…â–„â–‚â–„â–ƒâ–…â–„â–†â–‚â–ƒâ–†â–ˆâ–ƒâ–ƒâ–„â–„â–ƒâ–…â–â–ƒâ–…â–…
wandb:                train_loss/island â–ˆâ–ˆâ–†â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train_loss/total â–ˆâ–ˆâ–†â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                   train_step_acc â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:              trainer/global_step â–ƒâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:               val_last_step_loss â–ˆâ–ˆâ–†â–†â–„â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–…â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        valid_acc â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–„â–…â–†â–‡â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:  dream_loss/run_0/multi_target_0 14.1668
wandb:  dream_loss/run_0/multi_target_1 9.47863
wandb: dream_loss/run_0/multi_target_10 16.67657
wandb: dream_loss/run_0/multi_target_11 13.29363
wandb: dream_loss/run_0/multi_target_12 11.80378
wandb: dream_loss/run_0/multi_target_13 14.32061
wandb: dream_loss/run_0/multi_target_14 14.26779
wandb: dream_loss/run_0/multi_target_15 12.79976
wandb: dream_loss/run_0/multi_target_16 13.04121
wandb: dream_loss/run_0/multi_target_17 8.61067
wandb: dream_loss/run_0/multi_target_18 17.12799
wandb: dream_loss/run_0/multi_target_19 15.88834
wandb:  dream_loss/run_0/multi_target_2 13.89497
wandb: dream_loss/run_0/multi_target_20 16.51378
wandb: dream_loss/run_0/multi_target_21 20.03345
wandb: dream_loss/run_0/multi_target_22 11.67347
wandb: dream_loss/run_0/multi_target_23 15.01665
wandb: dream_loss/run_0/multi_target_24 11.09879
wandb: dream_loss/run_0/multi_target_25 20.63374
wandb: dream_loss/run_0/multi_target_26 9.31928
wandb: dream_loss/run_0/multi_target_27 16.35518
wandb: dream_loss/run_0/multi_target_28 33.7841
wandb: dream_loss/run_0/multi_target_29 13.13957
wandb:  dream_loss/run_0/multi_target_3 26.11104
wandb: dream_loss/run_0/multi_target_30 12.48651
wandb: dream_loss/run_0/multi_target_31 14.20266
wandb: dream_loss/run_0/multi_target_32 16.88737
wandb: dream_loss/run_0/multi_target_33 14.27306
wandb: dream_loss/run_0/multi_target_34 11.13257
wandb: dream_loss/run_0/multi_target_35 10.51321
wandb: dream_loss/run_0/multi_target_36 12.25273
wandb: dream_loss/run_0/multi_target_37 13.45705
wandb: dream_loss/run_0/multi_target_38 13.12244
wandb: dream_loss/run_0/multi_target_39 13.62083
wandb:  dream_loss/run_0/multi_target_4 11.69126
wandb: dream_loss/run_0/multi_target_40 19.65699
wandb: dream_loss/run_0/multi_target_41 13.84582
wandb: dream_loss/run_0/multi_target_42 8.70957
wandb: dream_loss/run_0/multi_target_43 12.96994
wandb: dream_loss/run_0/multi_target_44 12.80988
wandb: dream_loss/run_0/multi_target_45 18.39228
wandb: dream_loss/run_0/multi_target_46 19.67839
wandb: dream_loss/run_0/multi_target_47 6.61032
wandb: dream_loss/run_0/multi_target_48 22.28666
wandb: dream_loss/run_0/multi_target_49 15.85833
wandb:  dream_loss/run_0/multi_target_5 17.10132
wandb: dream_loss/run_0/multi_target_50 9.34007
wandb: dream_loss/run_0/multi_target_51 17.07647
wandb: dream_loss/run_0/multi_target_52 20.17235
wandb: dream_loss/run_0/multi_target_53 16.22359
wandb: dream_loss/run_0/multi_target_54 11.30261
wandb: dream_loss/run_0/multi_target_55 12.8186
wandb: dream_loss/run_0/multi_target_56 25.85941
wandb: dream_loss/run_0/multi_target_57 11.14206
wandb: dream_loss/run_0/multi_target_58 12.11416
wandb: dream_loss/run_0/multi_target_59 12.95824
wandb:  dream_loss/run_0/multi_target_6 30.67556
wandb: dream_loss/run_0/multi_target_60 14.26733
wandb: dream_loss/run_0/multi_target_61 17.52189
wandb: dream_loss/run_0/multi_target_62 15.04075
wandb: dream_loss/run_0/multi_target_63 9.99963
wandb: dream_loss/run_0/multi_target_64 15.24538
wandb: dream_loss/run_0/multi_target_65 6.67525
wandb: dream_loss/run_0/multi_target_66 16.0799
wandb: dream_loss/run_0/multi_target_67 18.2088
wandb: dream_loss/run_0/multi_target_68 7.57466
wandb: dream_loss/run_0/multi_target_69 10.21317
wandb:  dream_loss/run_0/multi_target_7 11.05866
wandb: dream_loss/run_0/multi_target_70 23.54176
wandb: dream_loss/run_0/multi_target_71 11.72146
wandb: dream_loss/run_0/multi_target_72 11.28096
wandb: dream_loss/run_0/multi_target_73 11.88539
wandb: dream_loss/run_0/multi_target_74 18.79441
wandb: dream_loss/run_0/multi_target_75 9.89171
wandb: dream_loss/run_0/multi_target_76 14.89382
wandb: dream_loss/run_0/multi_target_77 36.58467
wandb: dream_loss/run_0/multi_target_78 14.7119
wandb: dream_loss/run_0/multi_target_79 11.88815
wandb:  dream_loss/run_0/multi_target_8 18.42428
wandb: dream_loss/run_0/multi_target_80 24.73727
wandb: dream_loss/run_0/multi_target_81 14.23025
wandb: dream_loss/run_0/multi_target_82 11.91164
wandb: dream_loss/run_0/multi_target_83 31.22173
wandb: dream_loss/run_0/multi_target_84 13.78912
wandb: dream_loss/run_0/multi_target_85 10.8037
wandb: dream_loss/run_0/multi_target_86 8.47099
wandb: dream_loss/run_0/multi_target_87 18.65601
wandb: dream_loss/run_0/multi_target_88 11.65581
wandb: dream_loss/run_0/multi_target_89 16.26443
wandb:  dream_loss/run_0/multi_target_9 16.1839
wandb: dream_loss/run_0/multi_target_90 17.16613
wandb: dream_loss/run_0/multi_target_91 12.92678
wandb: dream_loss/run_0/multi_target_92 15.83919
wandb: dream_loss/run_0/multi_target_93 14.65422
wandb: dream_loss/run_0/multi_target_94 17.42378
wandb: dream_loss/run_0/multi_target_95 12.5812
wandb: dream_loss/run_0/multi_target_96 29.97414
wandb: dream_loss/run_0/multi_target_97 14.39812
wandb: dream_loss/run_0/multi_target_98 14.12911
wandb: dream_loss/run_0/multi_target_99 22.12629
wandb:                            epoch 0
wandb:                    negative_loss 63.27958
wandb:                    positive_loss 1.30931
wandb:                            ratio 10.0
wandb:                      rho_sigma_2 100.0
wandb:                            scale 20.0
wandb:           stats/collect_accuracy 0.54621
wandb:               stats/collect_loss 71.82688
wandb:                         test_acc 0.5352
wandb:                  test_loss_epoch 72.94353
wandb:                   test_loss_step 74.18647
wandb:                train_loss/island 64.41605
wandb:                 train_loss/total 64.41605
wandb:                   train_step_acc 0.76554
wandb:              trainer/global_step 2420
wandb:               val_last_step_loss 72.94353
wandb:                        valid_acc 0.5352
wandb: 
wandb: ğŸš€ View run exp_search_model_type_dla_latent_size_3_taq3ahfx_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/taq3ahfx
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 214 media file(s), 2208 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240422_180703-taq3ahfx/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240422_224835-jwmwu48w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_dla_latent_size_10_jwmwu48w_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/jwmwu48w
End of experiment: exp_search_model_type_dla_latent_size_3; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: exp_search_model_type_dla_latent_size_10; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=10, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='dla', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: dla
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: Generated:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_DLA [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ DLA                 â”‚ 32.6 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 32.6 M                                                                                                               
Non-trainable params: 0                                                                                                                
Total params: 32.6 M                                                                                                                   
Total estimated model params size (MB): 130                                                                                            
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mTESTING TASK 0, loop 0 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚     0.516700029373169     â”‚
â”‚      test_loss_epoch      â”‚    -283.72564697265625    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 274/274 0:00:34 â€¢ 0:00:00 8.11it/s   
Testing   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:44 â€¢ 0:00:00 23.40it/s  
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: hooking model during visualization to - [33mDEEP INVERSION[34m - task: 0, loop 1 [0m
[34mINFO: For layerloss DeepInversionFeatureLoss used with default value: [36m'0.1'[34m  [0m
[34mINFO: hooking model during visualization to - [33mVARIATION IMAGE REGULARIZATION[34m - task: 0, loop 1 [0m
[34mINFO: hooking model during visualization to - [33mL2 IMAGE REGULARIZATION[34m - task: 0, loop 1 [0m
[34mDREAMING DURING TASK: 0, loop 1 [0m
[36mVIS: Visualization for targets: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99} [0m
[34mVIS: No hook to image after forward and backward pass. [0m
[34mVIS: Optimizer set during visualization config: [32mAdam (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0
) [0m
[32mVIS: Image size before (up/down)sample - torch.Size([220, 3, 41, 41]) [0m
[32mVIS: Image size after (up/down)sample - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: ENABLE DREAM TRANSFORMS [0m
[32mVIS: Current in use image size - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: Number of images to visualize (may change in fast-dev-run): 22000; batch size: 220; per target: 220; batches to be generated: 100 [0m
[36mVIS: Using multitarget visualization [0m
[34mTime generating features: 01:42:42 [0m
[34mDREAMING END [0m
[34mSTARTING TASK 0, loop 1 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[31mINFO: SKIPPING ANY TRAINING at loop 1 [0m
[34mENDING TASK 0, loop 1 [0m
[34mTESTING TASK 0, loop 1 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚     0.516700029373169     â”‚
â”‚      test_loss_epoch      â”‚    -283.72564697265625    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Iteration: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 500/500 0:01:00 â€¢ 0:00:00 8.23it/s   
                                                                                         
                                                                                         
Testing    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:01 â€¢ 0:00:00 23.47it/s  
[34mINFO: Generated current run name: d23-04-2024_h03-28-09_jwmwu48w [0m
[31mWARNING: At loop 2 selected last epoch per task "0" because list index out of range. [0m
[31mWARNING: At loop 2 selected last epoch per task "0" because list index out of range. [0m

End of training.
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 10 dimensions.
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h03-28-09_jwmwu48w/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h03-28-09_jwmwu48w/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h03-28-09_jwmwu48w/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h03-28-09_jwmwu48w/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h03-28-09_jwmwu48w/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h03-28-09_jwmwu48w/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h03-28-09_jwmwu48w/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h03-28-09_jwmwu48w/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h03-28-09_jwmwu48w/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/DLA/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h03-28-09_jwmwu48w/plots/mean_dist_matrix_idx100
wandb: - 13.247 MB of 13.247 MB uploaded (2.705 MB deduped)wandb: \ 13.247 MB of 13.247 MB uploaded (2.705 MB deduped)wandb: | 13.247 MB of 13.247 MB uploaded (2.705 MB deduped)wandb: / 13.396 MB of 13.666 MB uploaded (2.707 MB deduped)wandb: - 13.396 MB of 13.666 MB uploaded (2.707 MB deduped)wandb: \ 13.666 MB of 13.666 MB uploaded (2.707 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 19.8%             
wandb: 
wandb: Run history:
wandb:  dream_loss/run_0/multi_target_0 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_1 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_10 â–ˆâ–…â–„â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_11 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_12 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_13 â–ˆâ–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_14 â–ˆâ–…â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_15 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_16 â–ˆâ–…â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_17 â–ˆâ–…â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_18 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_19 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_2 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_20 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_21 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_22 â–ˆâ–…â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_23 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_24 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_25 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_26 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_27 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_28 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_29 â–ˆâ–…â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_3 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_30 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_31 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_32 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_33 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_34 â–ˆâ–†â–„â–„â–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_35 â–ˆâ–…â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_36 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_37 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_38 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_39 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_4 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_40 â–ˆâ–…â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_41 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_42 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_43 â–ˆâ–…â–„â–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_44 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_45 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_46 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_47 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_48 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_49 â–ˆâ–…â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_5 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_50 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_51 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_52 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_53 â–ˆâ–…â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_54 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_55 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_56 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_57 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_58 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_59 â–ˆâ–…â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_6 â–ˆâ–…â–„â–ƒâ–…â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_60 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_61 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_62 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_63 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_64 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_65 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_66 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_67 â–ˆâ–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_68 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_69 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_7 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_70 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_71 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_72 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_73 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_74 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_75 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_76 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_77 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_78 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_79 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_8 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_80 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_81 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_82 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_83 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_84 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_85 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_86 â–ˆâ–…â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_87 â–ˆâ–…â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_88 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_89 â–ˆâ–…â–„â–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_9 â–ˆâ–…â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_90 â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_91 â–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_92 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_93 â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_94 â–ˆâ–…â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_95 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_96 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_97 â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_98 â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_99 â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    negative_loss â–‡â–ˆâ–†â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    positive_loss â–‡â–ˆâ–†â–†â–†â–„â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–
wandb:                            ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                      rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           stats/collect_accuracy â–ˆâ–†â–‚â–â–‚â–‚â–‚â–â–‚â–â–â–
wandb:               stats/collect_loss â–ƒâ–„â–ˆâ–…â–â–…â–„â–„â–„â–…â–„â–ƒ
wandb:                         test_acc â–â–
wandb:                  test_loss_epoch â–â–
wandb:                   test_loss_step â–ƒâ–†â–‚â–„â–„â–ƒâ–…â–ƒâ–„â–ˆâ–„â–â–„â–ƒâ–…â–…â–„â–„â–„â–…â–ƒâ–…â–„â–„â–„â–…â–…â–„â–†â–ˆâ–ƒâ–ƒâ–„â–‡â–‡â–…â–ƒâ–…â–„â–†
wandb:                train_loss/island â–ˆâ–‡â–†â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train_loss/total â–ˆâ–‡â–†â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                   train_step_acc â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:              trainer/global_step â–ƒâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:               val_last_step_loss â–ˆâ–‡â–‡â–†â–†â–…â–…â–ƒâ–„â–‚â–„â–ƒâ–‚â–ƒâ–…â–‚â–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        valid_acc â–â–â–‚â–‚â–â–‚â–‚â–„â–ƒâ–…â–ƒâ–„â–…â–…â–ƒâ–…â–†â–‡â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:  dream_loss/run_0/multi_target_0 1470.13269
wandb:  dream_loss/run_0/multi_target_1 1442.44226
wandb: dream_loss/run_0/multi_target_10 1770.74158
wandb: dream_loss/run_0/multi_target_11 1112.20044
wandb: dream_loss/run_0/multi_target_12 1361.01721
wandb: dream_loss/run_0/multi_target_13 1305.16357
wandb: dream_loss/run_0/multi_target_14 1338.36047
wandb: dream_loss/run_0/multi_target_15 1485.05627
wandb: dream_loss/run_0/multi_target_16 1582.74023
wandb: dream_loss/run_0/multi_target_17 1101.95459
wandb: dream_loss/run_0/multi_target_18 951.15662
wandb: dream_loss/run_0/multi_target_19 1176.45886
wandb:  dream_loss/run_0/multi_target_2 1155.48572
wandb: dream_loss/run_0/multi_target_20 1472.53394
wandb: dream_loss/run_0/multi_target_21 1429.91345
wandb: dream_loss/run_0/multi_target_22 1298.96179
wandb: dream_loss/run_0/multi_target_23 1025.00159
wandb: dream_loss/run_0/multi_target_24 1752.97156
wandb: dream_loss/run_0/multi_target_25 1297.20276
wandb: dream_loss/run_0/multi_target_26 1590.80334
wandb: dream_loss/run_0/multi_target_27 1212.63611
wandb: dream_loss/run_0/multi_target_28 2027.23315
wandb: dream_loss/run_0/multi_target_29 1840.12671
wandb:  dream_loss/run_0/multi_target_3 1374.64429
wandb: dream_loss/run_0/multi_target_30 1731.50757
wandb: dream_loss/run_0/multi_target_31 1176.20422
wandb: dream_loss/run_0/multi_target_32 1613.07544
wandb: dream_loss/run_0/multi_target_33 818.91003
wandb: dream_loss/run_0/multi_target_34 1183.96118
wandb: dream_loss/run_0/multi_target_35 1246.58093
wandb: dream_loss/run_0/multi_target_36 1529.80273
wandb: dream_loss/run_0/multi_target_37 1159.10779
wandb: dream_loss/run_0/multi_target_38 1152.06238
wandb: dream_loss/run_0/multi_target_39 1328.44617
wandb:  dream_loss/run_0/multi_target_4 1479.10608
wandb: dream_loss/run_0/multi_target_40 1581.57117
wandb: dream_loss/run_0/multi_target_41 1275.10706
wandb: dream_loss/run_0/multi_target_42 1265.8175
wandb: dream_loss/run_0/multi_target_43 1455.38586
wandb: dream_loss/run_0/multi_target_44 1038.21301
wandb: dream_loss/run_0/multi_target_45 1410.53137
wandb: dream_loss/run_0/multi_target_46 1330.54321
wandb: dream_loss/run_0/multi_target_47 937.62604
wandb: dream_loss/run_0/multi_target_48 1550.14978
wandb: dream_loss/run_0/multi_target_49 1359.72498
wandb:  dream_loss/run_0/multi_target_5 1354.2666
wandb: dream_loss/run_0/multi_target_50 984.42523
wandb: dream_loss/run_0/multi_target_51 1603.81836
wandb: dream_loss/run_0/multi_target_52 1415.38562
wandb: dream_loss/run_0/multi_target_53 1333.4436
wandb: dream_loss/run_0/multi_target_54 1682.21899
wandb: dream_loss/run_0/multi_target_55 824.9892
wandb: dream_loss/run_0/multi_target_56 1381.21313
wandb: dream_loss/run_0/multi_target_57 1304.11377
wandb: dream_loss/run_0/multi_target_58 1301.60608
wandb: dream_loss/run_0/multi_target_59 1550.25256
wandb:  dream_loss/run_0/multi_target_6 1854.26526
wandb: dream_loss/run_0/multi_target_60 990.8598
wandb: dream_loss/run_0/multi_target_61 1614.0177
wandb: dream_loss/run_0/multi_target_62 1175.05334
wandb: dream_loss/run_0/multi_target_63 1238.19177
wandb: dream_loss/run_0/multi_target_64 1022.57782
wandb: dream_loss/run_0/multi_target_65 1201.67456
wandb: dream_loss/run_0/multi_target_66 1216.92029
wandb: dream_loss/run_0/multi_target_67 1685.47339
wandb: dream_loss/run_0/multi_target_68 1350.81946
wandb: dream_loss/run_0/multi_target_69 1160.41418
wandb:  dream_loss/run_0/multi_target_7 1578.08301
wandb: dream_loss/run_0/multi_target_70 1171.09778
wandb: dream_loss/run_0/multi_target_71 1004.21112
wandb: dream_loss/run_0/multi_target_72 1018.70728
wandb: dream_loss/run_0/multi_target_73 1192.18701
wandb: dream_loss/run_0/multi_target_74 1305.16357
wandb: dream_loss/run_0/multi_target_75 1287.12598
wandb: dream_loss/run_0/multi_target_76 1752.97803
wandb: dream_loss/run_0/multi_target_77 1788.26392
wandb: dream_loss/run_0/multi_target_78 1411.57703
wandb: dream_loss/run_0/multi_target_79 1284.07654
wandb:  dream_loss/run_0/multi_target_8 1433.60596
wandb: dream_loss/run_0/multi_target_80 1312.70618
wandb: dream_loss/run_0/multi_target_81 1068.75586
wandb: dream_loss/run_0/multi_target_82 1131.09448
wandb: dream_loss/run_0/multi_target_83 2031.17505
wandb: dream_loss/run_0/multi_target_84 991.03082
wandb: dream_loss/run_0/multi_target_85 1317.61169
wandb: dream_loss/run_0/multi_target_86 1501.79138
wandb: dream_loss/run_0/multi_target_87 1586.11279
wandb: dream_loss/run_0/multi_target_88 1411.53345
wandb: dream_loss/run_0/multi_target_89 1560.78687
wandb:  dream_loss/run_0/multi_target_9 1461.72546
wandb: dream_loss/run_0/multi_target_90 1258.89929
wandb: dream_loss/run_0/multi_target_91 929.96564
wandb: dream_loss/run_0/multi_target_92 1122.20117
wandb: dream_loss/run_0/multi_target_93 1410.37122
wandb: dream_loss/run_0/multi_target_94 1448.40845
wandb: dream_loss/run_0/multi_target_95 1519.80566
wandb: dream_loss/run_0/multi_target_96 1601.78613
wandb: dream_loss/run_0/multi_target_97 1205.68591
wandb: dream_loss/run_0/multi_target_98 1618.79773
wandb: dream_loss/run_0/multi_target_99 1376.75586
wandb:                            epoch 0
wandb:                    negative_loss -395.59747
wandb:                    positive_loss 19.02351
wandb:                            ratio 10.0
wandb:                      rho_sigma_2 100.0
wandb:                            scale 20.0
wandb:           stats/collect_accuracy 0.52462
wandb:               stats/collect_loss -299.83948
wandb:                         test_acc 0.5167
wandb:                  test_loss_epoch -283.72565
wandb:                   test_loss_step -261.53696
wandb:                train_loss/island -377.14105
wandb:                 train_loss/total -377.14105
wandb:                   train_step_acc 0.72516
wandb:              trainer/global_step 2420
wandb:               val_last_step_loss -283.72565
wandb:                        valid_acc 0.5167
wandb: 
wandb: ğŸš€ View run exp_search_model_type_dla_latent_size_10_jwmwu48w_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/jwmwu48w
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 213 media file(s), 2208 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240422_224835-jwmwu48w/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240423_033018-6s8cnpyw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_dla_latent_size_20_6s8cnpyw_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/6s8cnpyw
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: exp_search_model_type_dla_latent_size_10; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: exp_search_model_type_dla_latent_size_20; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='dla', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: dla
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_DLA [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ DLA                 â”‚ 32.6 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 32.6 M                                                                                                               
Non-trainable params: 0                                                                                                                
Total params: 32.6 M                                                                                                                   
Total estimated model params size (MB): 130                                                                                            
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
Epoch 0    â”                                        9/274 0:00:01 â€¢ 0:00:38 7.11it/s loss: 3.08e+21 v_num: npyw 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 68, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 758, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.021 MB of 0.042 MB uploaded (0.002 MB deduped)wandb: - 0.021 MB of 0.042 MB uploaded (0.002 MB deduped)wandb: \ 0.042 MB of 0.042 MB uploaded (0.002 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 4.7%             
wandb: ğŸš€ View run exp_search_model_type_dla_latent_size_20_6s8cnpyw_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/6s8cnpyw
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240423_033018-6s8cnpyw/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240423_033101-zj9jdel5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_dla_latent_size_30_zj9jdel5_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/zj9jdel5
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: exp_search_model_type_dla_latent_size_20; repeat 1/1
Clearing gpu cache and invoking garbage collector
[31mWARNING:	dreaming images were not flushed by wandb. [0m
Done
Running experiment: exp_search_model_type_dla_latent_size_30; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='dla', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: dla
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_DLA [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ DLA                 â”‚ 32.6 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 32.6 M                                                                                                               
Non-trainable params: 0                                                                                                                
Total params: 32.6 M                                                                                                                   
Total estimated model params size (MB): 130                                                                                            
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
Epoch 0    â•¸                                        6/274 0:00:01 â€¢ 0:00:38 7.11it/s loss: 4e+27 v_num: del5 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 68, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 758, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.039 MB uploadedwandb: - 0.017 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run exp_search_model_type_dla_latent_size_30_zj9jdel5_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/zj9jdel5
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240423_033101-zj9jdel5/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240423_033140-30ob0n88
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_vgg_latent_size_3_30ob0n88_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/30ob0n88
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: exp_search_model_type_dla_latent_size_30; repeat 1/1
Clearing gpu cache and invoking garbage collector
[31mWARNING:	dreaming images were not flushed by wandb. [0m
Done
Running experiment: exp_search_model_type_vgg_latent_size_3; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=3, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='vgg', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: vgg
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                                                
Non-trainable params: 0                                                                                                                
Total params: 9.2 M                                                                                                                    
Total estimated model params size (MB): 36                                                                                             
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mTESTING TASK 0, loop 0 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5083000063896179     â”‚
â”‚      test_loss_epoch      â”‚     74.1539077758789      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 274/274 0:00:17 â€¢ 0:00:00 15.76it/s  
Testing   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:43 â€¢ 0:00:00 42.15it/s  
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: hooking model during visualization to - [33mDEEP INVERSION[34m - task: 0, loop 1 [0m
[34mINFO: For layerloss DeepInversionFeatureLoss used with default value: [36m'0.1'[34m  [0m
[34mINFO: hooking model during visualization to - [33mVARIATION IMAGE REGULARIZATION[34m - task: 0, loop 1 [0m
[34mINFO: hooking model during visualization to - [33mL2 IMAGE REGULARIZATION[34m - task: 0, loop 1 [0m
[34mDREAMING DURING TASK: 0, loop 1 [0m
[36mVIS: Visualization for targets: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99} [0m
[34mVIS: No hook to image after forward and backward pass. [0m
[34mVIS: Optimizer set during visualization config: [32mAdam (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0
) [0m
[32mVIS: Image size before (up/down)sample - torch.Size([220, 3, 41, 41]) [0m
[32mVIS: Image size after (up/down)sample - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: ENABLE DREAM TRANSFORMS [0m
[32mVIS: Current in use image size - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: Number of images to visualize (may change in fast-dev-run): 22000; batch size: 220; per target: 220; batches to be generated: 100 [0m
[36mVIS: Using multitarget visualization [0m
[34mTime generating features: 00:18:53 [0m
[34mDREAMING END [0m
[34mSTARTING TASK 0, loop 1 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[31mINFO: SKIPPING ANY TRAINING at loop 1 [0m
[34mENDING TASK 0, loop 1 [0m
[34mTESTING TASK 0, loop 1 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5083000063896179     â”‚
â”‚      test_loss_epoch      â”‚     74.1539077758789      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Iteration: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 500/500 0:00:10 â€¢ 0:00:00 47.18it/s  
                                                                                         
                                                                                         
Testing    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:01 â€¢ 0:00:00 42.93it/s  
[34mINFO: Generated current run name: d23-04-2024_h05-19-13_30ob0n88 [0m
[31mWARNING: At loop 2 selected last epoch per task "0" because list index out of range. [0m
[31mWARNING: At loop 2 selected last epoch per task "0" because list index out of range. [0m

End of training.
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h05-19-13_30ob0n88/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h05-19-13_30ob0n88/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h05-19-13_30ob0n88/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h05-19-13_30ob0n88/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h05-19-13_30ob0n88/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h05-19-13_30ob0n88/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h05-19-13_30ob0n88/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h05-19-13_30ob0n88/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h05-19-13_30ob0n88/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h05-19-13_30ob0n88/plots/mean_dist_matrix_idx100
wandb: - 14.277 MB of 14.277 MB uploaded (3.140 MB deduped)wandb: \ 14.277 MB of 14.277 MB uploaded (3.140 MB deduped)wandb: | 14.277 MB of 14.277 MB uploaded (3.140 MB deduped)wandb: / 14.277 MB of 14.277 MB uploaded (3.140 MB deduped)wandb: - 14.427 MB of 14.697 MB uploaded (3.142 MB deduped)wandb: \ 14.697 MB of 14.697 MB uploaded (3.142 MB deduped)wandb: | 14.697 MB of 14.697 MB uploaded (3.142 MB deduped)wandb: / 14.697 MB of 14.697 MB uploaded (3.142 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 21.4%             
wandb: 
wandb: Run history:
wandb:  dream_loss/run_0/multi_target_0 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_1 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_10 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_11 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_12 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_13 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_14 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_15 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_16 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_17 â–ˆâ–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_18 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_19 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_2 â–ˆâ–‚â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_20 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_21 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_22 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_23 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_24 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_25 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_26 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_27 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_28 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_29 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_3 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_30 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_31 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_32 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_33 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_34 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_35 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_36 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_37 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_38 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_39 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_4 â–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_40 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_41 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_42 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_43 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_44 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_45 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_46 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_47 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_48 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_49 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_5 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_50 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_51 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_52 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_53 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_54 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_55 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_56 â–ˆâ–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_57 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_58 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_59 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_6 â–ˆâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_60 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_61 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_62 â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_63 â–ˆâ–ƒâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_64 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_65 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_66 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_67 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_68 â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_69 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_7 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_70 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_71 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_72 â–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_73 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_74 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_75 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_76 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_77 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_78 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_79 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_8 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_80 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_81 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_82 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_83 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_84 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_85 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_86 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_87 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_88 â–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_89 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_9 â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_90 â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_91 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_92 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_93 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_94 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_95 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_96 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_97 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_98 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_99 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    negative_loss â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    positive_loss â–ˆâ–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                      rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           stats/collect_accuracy â–ˆâ–ˆâ–ƒâ–„â–ƒâ–â–‚â–â–‚â–‚â–ƒâ–ƒ
wandb:               stats/collect_loss â–‚â–…â–†â–ˆâ–‚â–â–ƒâ–„â–…â–„â–ƒâ–‚
wandb:                         test_acc â–â–
wandb:                  test_loss_epoch â–â–
wandb:                   test_loss_step â–‚â–…â–‚â–„â–„â–‚â–ƒâ–„â–‚â–‡â–ƒâ–â–…â–ƒâ–„â–†â–„â–ƒâ–„â–„â–‚â–†â–‚â–„â–ƒâ–ˆâ–ƒâ–‚â–‡â–‡â–…â–ƒâ–…â–„â–ƒâ–†â–‚â–‚â–„â–…
wandb:                train_loss/island â–ˆâ–‡â–…â–…â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train_loss/total â–ˆâ–‡â–…â–…â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                   train_step_acc â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:              trainer/global_step â–ƒâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:               val_last_step_loss â–ˆâ–‡â–‡â–‡â–…â–ƒâ–ƒâ–„â–‡â–ƒâ–ƒâ–‚â–‚â–â–ƒâ–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        valid_acc â–â–â–â–‚â–‚â–ƒâ–„â–ƒâ–‚â–„â–„â–…â–†â–†â–†â–‡â–‡â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:  dream_loss/run_0/multi_target_0 13.77625
wandb:  dream_loss/run_0/multi_target_1 7.70454
wandb: dream_loss/run_0/multi_target_10 12.01727
wandb: dream_loss/run_0/multi_target_11 9.3406
wandb: dream_loss/run_0/multi_target_12 7.51874
wandb: dream_loss/run_0/multi_target_13 11.08065
wandb: dream_loss/run_0/multi_target_14 9.79745
wandb: dream_loss/run_0/multi_target_15 9.48419
wandb: dream_loss/run_0/multi_target_16 11.49794
wandb: dream_loss/run_0/multi_target_17 7.6715
wandb: dream_loss/run_0/multi_target_18 10.8697
wandb: dream_loss/run_0/multi_target_19 15.28542
wandb:  dream_loss/run_0/multi_target_2 10.29583
wandb: dream_loss/run_0/multi_target_20 12.87077
wandb: dream_loss/run_0/multi_target_21 20.58245
wandb: dream_loss/run_0/multi_target_22 10.92358
wandb: dream_loss/run_0/multi_target_23 27.65285
wandb: dream_loss/run_0/multi_target_24 10.17175
wandb: dream_loss/run_0/multi_target_25 17.03094
wandb: dream_loss/run_0/multi_target_26 7.98023
wandb: dream_loss/run_0/multi_target_27 22.74875
wandb: dream_loss/run_0/multi_target_28 29.20339
wandb: dream_loss/run_0/multi_target_29 11.69634
wandb:  dream_loss/run_0/multi_target_3 24.82579
wandb: dream_loss/run_0/multi_target_30 12.2779
wandb: dream_loss/run_0/multi_target_31 8.42088
wandb: dream_loss/run_0/multi_target_32 11.52198
wandb: dream_loss/run_0/multi_target_33 8.74569
wandb: dream_loss/run_0/multi_target_34 8.18843
wandb: dream_loss/run_0/multi_target_35 8.96632
wandb: dream_loss/run_0/multi_target_36 10.45825
wandb: dream_loss/run_0/multi_target_37 7.37226
wandb: dream_loss/run_0/multi_target_38 10.92743
wandb: dream_loss/run_0/multi_target_39 14.0171
wandb:  dream_loss/run_0/multi_target_4 7.90101
wandb: dream_loss/run_0/multi_target_40 11.53091
wandb: dream_loss/run_0/multi_target_41 9.41402
wandb: dream_loss/run_0/multi_target_42 7.53812
wandb: dream_loss/run_0/multi_target_43 17.77557
wandb: dream_loss/run_0/multi_target_44 12.74789
wandb: dream_loss/run_0/multi_target_45 17.85391
wandb: dream_loss/run_0/multi_target_46 23.79245
wandb: dream_loss/run_0/multi_target_47 5.30486
wandb: dream_loss/run_0/multi_target_48 18.96341
wandb: dream_loss/run_0/multi_target_49 10.22466
wandb:  dream_loss/run_0/multi_target_5 12.76885
wandb: dream_loss/run_0/multi_target_50 6.30185
wandb: dream_loss/run_0/multi_target_51 12.72514
wandb: dream_loss/run_0/multi_target_52 17.62633
wandb: dream_loss/run_0/multi_target_53 10.87621
wandb: dream_loss/run_0/multi_target_54 10.26662
wandb: dream_loss/run_0/multi_target_55 26.17893
wandb: dream_loss/run_0/multi_target_56 24.02441
wandb: dream_loss/run_0/multi_target_57 8.14869
wandb: dream_loss/run_0/multi_target_58 10.52964
wandb: dream_loss/run_0/multi_target_59 9.19572
wandb:  dream_loss/run_0/multi_target_6 22.95267
wandb: dream_loss/run_0/multi_target_60 6.14463
wandb: dream_loss/run_0/multi_target_61 9.98029
wandb: dream_loss/run_0/multi_target_62 12.86504
wandb: dream_loss/run_0/multi_target_63 14.9993
wandb: dream_loss/run_0/multi_target_64 11.07019
wandb: dream_loss/run_0/multi_target_65 6.02169
wandb: dream_loss/run_0/multi_target_66 12.04694
wandb: dream_loss/run_0/multi_target_67 13.56283
wandb: dream_loss/run_0/multi_target_68 8.00636
wandb: dream_loss/run_0/multi_target_69 8.24407
wandb:  dream_loss/run_0/multi_target_7 8.93313
wandb: dream_loss/run_0/multi_target_70 15.06062
wandb: dream_loss/run_0/multi_target_71 10.34462
wandb: dream_loss/run_0/multi_target_72 10.86281
wandb: dream_loss/run_0/multi_target_73 8.56461
wandb: dream_loss/run_0/multi_target_74 14.93985
wandb: dream_loss/run_0/multi_target_75 8.54091
wandb: dream_loss/run_0/multi_target_76 11.49784
wandb: dream_loss/run_0/multi_target_77 32.42606
wandb: dream_loss/run_0/multi_target_78 11.49455
wandb: dream_loss/run_0/multi_target_79 10.04601
wandb:  dream_loss/run_0/multi_target_8 12.26987
wandb: dream_loss/run_0/multi_target_80 18.19049
wandb: dream_loss/run_0/multi_target_81 11.80268
wandb: dream_loss/run_0/multi_target_82 11.85189
wandb: dream_loss/run_0/multi_target_83 25.95681
wandb: dream_loss/run_0/multi_target_84 11.8975
wandb: dream_loss/run_0/multi_target_85 10.19141
wandb: dream_loss/run_0/multi_target_86 7.6072
wandb: dream_loss/run_0/multi_target_87 13.05354
wandb: dream_loss/run_0/multi_target_88 8.95916
wandb: dream_loss/run_0/multi_target_89 10.86302
wandb:  dream_loss/run_0/multi_target_9 14.16115
wandb: dream_loss/run_0/multi_target_90 20.32898
wandb: dream_loss/run_0/multi_target_91 8.64754
wandb: dream_loss/run_0/multi_target_92 10.41722
wandb: dream_loss/run_0/multi_target_93 7.6376
wandb: dream_loss/run_0/multi_target_94 13.52124
wandb: dream_loss/run_0/multi_target_95 15.15996
wandb: dream_loss/run_0/multi_target_96 20.72478
wandb: dream_loss/run_0/multi_target_97 13.69132
wandb: dream_loss/run_0/multi_target_98 11.24189
wandb: dream_loss/run_0/multi_target_99 16.59198
wandb:                            epoch 0
wandb:                    negative_loss 63.04586
wandb:                    positive_loss 1.11612
wandb:                            ratio 10.0
wandb:                      rho_sigma_2 100.0
wandb:                            scale 20.0
wandb:           stats/collect_accuracy 0.51515
wandb:               stats/collect_loss 72.42027
wandb:                         test_acc 0.5083
wandb:                  test_loss_epoch 74.15391
wandb:                   test_loss_step 75.40408
wandb:                train_loss/island 65.39314
wandb:                 train_loss/total 65.39314
wandb:                   train_step_acc 0.8178
wandb:              trainer/global_step 2420
wandb:               val_last_step_loss 74.15391
wandb:                        valid_acc 0.5083
wandb: 
wandb: ğŸš€ View run exp_search_model_type_vgg_latent_size_3_30ob0n88_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/30ob0n88
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 214 media file(s), 2208 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240423_033140-30ob0n88/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240423_052027-odzruxsg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_vgg_latent_size_10_odzruxsg_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/odzruxsg
End of experiment: exp_search_model_type_vgg_latent_size_3; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: exp_search_model_type_vgg_latent_size_10; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=10, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='vgg', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: vgg
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: Generated:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                                                
Non-trainable params: 0                                                                                                                
Total params: 9.2 M                                                                                                                    
Total estimated model params size (MB): 36                                                                                             
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
Epoch 0    â”                                        10/274 0:00:00 â€¢ 0:00:19 14.26it/s loss: 2e+24 v_num: uxsg 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 68, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 758, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.044 MB uploadedwandb: | 0.017 MB of 0.044 MB uploadedwandb: / 0.044 MB of 0.044 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run exp_search_model_type_vgg_latent_size_10_odzruxsg_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/odzruxsg
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240423_052027-odzruxsg/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240423_052103-r6ijrq2f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_vgg_latent_size_20_r6ijrq2f_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/r6ijrq2f
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: exp_search_model_type_vgg_latent_size_10; repeat 1/1
Clearing gpu cache and invoking garbage collector
[31mWARNING:	dreaming images were not flushed by wandb. [0m
Done
Running experiment: exp_search_model_type_vgg_latent_size_20; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='vgg', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: vgg
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                                                
Non-trainable params: 0                                                                                                                
Total params: 9.2 M                                                                                                                    
Total estimated model params size (MB): 36                                                                                             
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
Epoch 0    â•¸                                        6/274 0:00:00 â€¢ 0:00:19 14.76it/s loss: 2.09e+31 v_num: rq2f 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 68, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 758, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.039 MB uploadedwandb: / 0.017 MB of 0.039 MB uploadedwandb: - 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run exp_search_model_type_vgg_latent_size_20_r6ijrq2f_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/r6ijrq2f
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240423_052103-r6ijrq2f/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240423_052138-00v86gyg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_vgg_latent_size_30_00v86gyg_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/00v86gyg
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: exp_search_model_type_vgg_latent_size_20; repeat 1/1
Clearing gpu cache and invoking garbage collector
[31mWARNING:	dreaming images were not flushed by wandb. [0m
Done
Running experiment: exp_search_model_type_vgg_latent_size_30; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='vgg', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: vgg
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                                                
Non-trainable params: 0                                                                                                                
Total params: 9.2 M                                                                                                                    
Total estimated model params size (MB): 36                                                                                             
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
Epoch 0    â•¸                                        6/274 0:00:00 â€¢ 0:00:20 13.64it/s loss: 6.67e+22 v_num: 6gyg 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 68, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 758, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.039 MB uploadedwandb: | 0.017 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run exp_search_model_type_vgg_latent_size_30_00v86gyg_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/00v86gyg
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240423_052138-00v86gyg/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240423_052214-i97l5jnx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_custom-resnet34_latent_size_3_i97l5jnx_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/i97l5jnx
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: exp_search_model_type_vgg_latent_size_30; repeat 1/1
Clearing gpu cache and invoking garbage collector
[31mWARNING:	dreaming images were not flushed by wandb. [0m
Done
Running experiment: exp_search_model_type_custom-resnet34_latent_size_3; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=3, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='custom-resnet34', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: custom-resnet34
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_CustomResNet34 [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ CustomResNet34      â”‚ 21.3 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 21.3 M                                                                                                               
Non-trainable params: 0                                                                                                                
Total params: 21.3 M                                                                                                                   
Total estimated model params size (MB): 85                                                                                             
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mTESTING TASK 0, loop 0 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5641999840736389     â”‚
â”‚      test_loss_epoch      â”‚     72.44647216796875     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 274/274 0:00:33 â€¢ 0:00:00 8.24it/s   
Testing   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:50 â€¢ 0:00:00 23.37it/s  
wandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.
wandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.
wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.
wandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.
wandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: hooking model during visualization to - [33mDEEP INVERSION[34m - task: 0, loop 1 [0m
[34mINFO: For layerloss DeepInversionFeatureLoss used with default value: [36m'0.1'[34m  [0m
[34mINFO: hooking model during visualization to - [33mVARIATION IMAGE REGULARIZATION[34m - task: 0, loop 1 [0m
[34mINFO: hooking model during visualization to - [33mL2 IMAGE REGULARIZATION[34m - task: 0, loop 1 [0m
[34mDREAMING DURING TASK: 0, loop 1 [0m
[36mVIS: Visualization for targets: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99} [0m
[34mVIS: No hook to image after forward and backward pass. [0m
[34mVIS: Optimizer set during visualization config: [32mAdam (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0
) [0m
[32mVIS: Image size before (up/down)sample - torch.Size([220, 3, 41, 41]) [0m
[32mVIS: Image size after (up/down)sample - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: ENABLE DREAM TRANSFORMS [0m
[32mVIS: Current in use image size - torch.Size([220, 3, 32, 32]) [0m
[36mVIS: Number of images to visualize (may change in fast-dev-run): 22000; batch size: 220; per target: 220; batches to be generated: 100 [0m
[36mVIS: Using multitarget visualization [0m
[34mTime generating features: 01:39:35 [0m
[34mDREAMING END [0m
[34mSTARTING TASK 0, loop 1 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[31mINFO: SKIPPING ANY TRAINING at loop 1 [0m
[34mENDING TASK 0, loop 1 [0m
[34mTESTING TASK 0, loop 1 [0m
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5641999840736389     â”‚
â”‚      test_loss_epoch      â”‚     72.44647216796875     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Iteration: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 500/500 0:00:58 â€¢ 0:00:00 8.59it/s   
                                                                                         
                                                                                         
Testing    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46   0:00:01 â€¢ 0:00:00 23.85it/s  
[34mINFO: Generated current run name: d23-04-2024_h09-55-53_i97l5jnx [0m
[31mWARNING: At loop 2 selected last epoch per task "0" because list index out of range. [0m
[31mWARNING: At loop 2 selected last epoch per task "0" because list index out of range. [0m

End of training.
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h09-55-53_i97l5jnx/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h09-55-53_i97l5jnx/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h09-55-53_i97l5jnx/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h09-55-53_i97l5jnx/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h09-55-53_i97l5jnx/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h09-55-53_i97l5jnx/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h09-55-53_i97l5jnx/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h09-55-53_i97l5jnx/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h09-55-53_i97l5jnx/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/CustomResNet34/layerloss/cfg_deep_inversion/regularization/var/l2/model_optim_type-sgd/-deep_inversion/image_reg_var-image_reg_l2/d23-04-2024_h09-55-53_i97l5jnx/plots/mean_dist_matrix_idx100
wandb: - 13.956 MB of 13.956 MB uploaded (2.929 MB deduped)wandb: \ 13.956 MB of 13.956 MB uploaded (2.929 MB deduped)wandb: | 13.956 MB of 13.956 MB uploaded (2.929 MB deduped)wandb: / 14.106 MB of 14.378 MB uploaded (2.931 MB deduped)wandb: - 14.106 MB of 14.378 MB uploaded (2.931 MB deduped)wandb: \ 14.378 MB of 14.378 MB uploaded (2.931 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 20.4%             
wandb: 
wandb: Run history:
wandb:  dream_loss/run_0/multi_target_0 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_1 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_10 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_11 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_12 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_13 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_14 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_15 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_16 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_17 â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_18 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_19 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_2 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_20 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_21 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_22 â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_23 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_24 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_25 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_26 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_27 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_28 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_29 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_3 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_30 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_31 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_32 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_33 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_34 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_35 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_36 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_37 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_38 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_39 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_4 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_40 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_41 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_42 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_43 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_44 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_45 â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_46 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_47 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_48 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_49 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_5 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_50 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_51 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_52 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_53 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_54 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_55 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_56 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_57 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_58 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_59 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_6 â–ˆâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_60 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_61 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_62 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_63 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_64 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_65 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_66 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_67 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_68 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_69 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_7 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_70 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_71 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_72 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_73 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_74 â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_75 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_76 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_77 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_78 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_79 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_8 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_80 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_81 â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_82 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_83 â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_84 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_85 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_86 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_87 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_88 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_89 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  dream_loss/run_0/multi_target_9 â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_90 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_91 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_92 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_93 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_94 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_95 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_96 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_97 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_98 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: dream_loss/run_0/multi_target_99 â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    negative_loss â–ˆâ–ˆâ–‡â–†â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    positive_loss â–ˆâ–‡â–‡â–†â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                      rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                            scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           stats/collect_accuracy â–â–ƒâ–„â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:               stats/collect_loss â–â–…â–„â–â–„â–ƒâ–…â–ƒâ–ˆâ–ƒâ–ƒâ–‚
wandb:                         test_acc â–â–
wandb:                  test_loss_epoch â–â–
wandb:                   test_loss_step â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–ˆâ–â–â–‚â–ƒâ–â–…â–â–ƒâ–ƒâ–ƒâ–â–â–‚â–‚â–‚â–‡â–ƒâ–â–…â–ˆâ–…â–‚â–‚â–ƒâ–„â–…â–â–‚â–ƒâ–…
wandb:                train_loss/island â–ˆâ–ˆâ–†â–…â–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 train_loss/total â–ˆâ–ˆâ–†â–…â–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                   train_step_acc â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:              trainer/global_step â–ƒâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:               val_last_step_loss â–ˆâ–ˆâ–‡â–…â–„â–…â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–â–‚â–ƒâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        valid_acc â–â–â–â–‚â–‚â–‚â–ƒâ–„â–„â–…â–…â–…â–…â–†â–…â–‡â–‡â–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:  dream_loss/run_0/multi_target_0 96.77176
wandb:  dream_loss/run_0/multi_target_1 27.22017
wandb: dream_loss/run_0/multi_target_10 31.6396
wandb: dream_loss/run_0/multi_target_11 24.40273
wandb: dream_loss/run_0/multi_target_12 31.34008
wandb: dream_loss/run_0/multi_target_13 28.0525
wandb: dream_loss/run_0/multi_target_14 28.22636
wandb: dream_loss/run_0/multi_target_15 25.994
wandb: dream_loss/run_0/multi_target_16 32.60344
wandb: dream_loss/run_0/multi_target_17 27.13877
wandb: dream_loss/run_0/multi_target_18 30.8385
wandb: dream_loss/run_0/multi_target_19 28.65453
wandb:  dream_loss/run_0/multi_target_2 31.41199
wandb: dream_loss/run_0/multi_target_20 33.5421
wandb: dream_loss/run_0/multi_target_21 34.43302
wandb: dream_loss/run_0/multi_target_22 35.98733
wandb: dream_loss/run_0/multi_target_23 39.48873
wandb: dream_loss/run_0/multi_target_24 31.4083
wandb: dream_loss/run_0/multi_target_25 31.98884
wandb: dream_loss/run_0/multi_target_26 26.11649
wandb: dream_loss/run_0/multi_target_27 32.99847
wandb: dream_loss/run_0/multi_target_28 156.30782
wandb: dream_loss/run_0/multi_target_29 26.42516
wandb:  dream_loss/run_0/multi_target_3 43.96376
wandb: dream_loss/run_0/multi_target_30 32.53587
wandb: dream_loss/run_0/multi_target_31 22.72392
wandb: dream_loss/run_0/multi_target_32 36.48658
wandb: dream_loss/run_0/multi_target_33 21.74898
wandb: dream_loss/run_0/multi_target_34 23.65333
wandb: dream_loss/run_0/multi_target_35 20.27092
wandb: dream_loss/run_0/multi_target_36 27.5124
wandb: dream_loss/run_0/multi_target_37 25.71102
wandb: dream_loss/run_0/multi_target_38 29.64185
wandb: dream_loss/run_0/multi_target_39 23.37538
wandb:  dream_loss/run_0/multi_target_4 25.29052
wandb: dream_loss/run_0/multi_target_40 44.06543
wandb: dream_loss/run_0/multi_target_41 21.1812
wandb: dream_loss/run_0/multi_target_42 24.39967
wandb: dream_loss/run_0/multi_target_43 30.27958
wandb: dream_loss/run_0/multi_target_44 25.89467
wandb: dream_loss/run_0/multi_target_45 34.06577
wandb: dream_loss/run_0/multi_target_46 36.33227
wandb: dream_loss/run_0/multi_target_47 18.34969
wandb: dream_loss/run_0/multi_target_48 51.12686
wandb: dream_loss/run_0/multi_target_49 37.24553
wandb:  dream_loss/run_0/multi_target_5 34.31927
wandb: dream_loss/run_0/multi_target_50 19.05982
wandb: dream_loss/run_0/multi_target_51 28.00455
wandb: dream_loss/run_0/multi_target_52 31.25631
wandb: dream_loss/run_0/multi_target_53 25.05009
wandb: dream_loss/run_0/multi_target_54 24.57343
wandb: dream_loss/run_0/multi_target_55 25.59476
wandb: dream_loss/run_0/multi_target_56 42.94886
wandb: dream_loss/run_0/multi_target_57 22.13997
wandb: dream_loss/run_0/multi_target_58 24.36187
wandb: dream_loss/run_0/multi_target_59 28.76767
wandb:  dream_loss/run_0/multi_target_6 54.8527
wandb: dream_loss/run_0/multi_target_60 21.80004
wandb: dream_loss/run_0/multi_target_61 33.33899
wandb: dream_loss/run_0/multi_target_62 26.98038
wandb: dream_loss/run_0/multi_target_63 37.32609
wandb: dream_loss/run_0/multi_target_64 30.73582
wandb: dream_loss/run_0/multi_target_65 32.8617
wandb: dream_loss/run_0/multi_target_66 26.18007
wandb: dream_loss/run_0/multi_target_67 29.53725
wandb: dream_loss/run_0/multi_target_68 23.4924
wandb: dream_loss/run_0/multi_target_69 22.67382
wandb:  dream_loss/run_0/multi_target_7 28.42102
wandb: dream_loss/run_0/multi_target_70 29.69074
wandb: dream_loss/run_0/multi_target_71 18.48074
wandb: dream_loss/run_0/multi_target_72 35.32507
wandb: dream_loss/run_0/multi_target_73 24.60809
wandb: dream_loss/run_0/multi_target_74 30.735
wandb: dream_loss/run_0/multi_target_75 21.29608
wandb: dream_loss/run_0/multi_target_76 28.93569
wandb: dream_loss/run_0/multi_target_77 114.92232
wandb: dream_loss/run_0/multi_target_78 29.50303
wandb: dream_loss/run_0/multi_target_79 24.35311
wandb:  dream_loss/run_0/multi_target_8 29.21936
wandb: dream_loss/run_0/multi_target_80 47.9872
wandb: dream_loss/run_0/multi_target_81 28.48254
wandb: dream_loss/run_0/multi_target_82 32.27737
wandb: dream_loss/run_0/multi_target_83 51.32775
wandb: dream_loss/run_0/multi_target_84 26.27449
wandb: dream_loss/run_0/multi_target_85 27.28572
wandb: dream_loss/run_0/multi_target_86 20.22931
wandb: dream_loss/run_0/multi_target_87 33.00791
wandb: dream_loss/run_0/multi_target_88 29.36572
wandb: dream_loss/run_0/multi_target_89 34.46587
wandb:  dream_loss/run_0/multi_target_9 28.24462
wandb: dream_loss/run_0/multi_target_90 36.91493
wandb: dream_loss/run_0/multi_target_91 30.92474
wandb: dream_loss/run_0/multi_target_92 28.35741
wandb: dream_loss/run_0/multi_target_93 26.50223
wandb: dream_loss/run_0/multi_target_94 25.91307
wandb: dream_loss/run_0/multi_target_95 26.79781
wandb: dream_loss/run_0/multi_target_96 48.6547
wandb: dream_loss/run_0/multi_target_97 25.21387
wandb: dream_loss/run_0/multi_target_98 29.49822
wandb: dream_loss/run_0/multi_target_99 36.56806
wandb:                            epoch 0
wandb:                    negative_loss 62.96398
wandb:                    positive_loss 1.05526
wandb:                            ratio 10.0
wandb:                      rho_sigma_2 100.0
wandb:                            scale 20.0
wandb:           stats/collect_accuracy 0.5803
wandb:               stats/collect_loss 71.22688
wandb:                         test_acc 0.5642
wandb:                  test_loss_epoch 72.44647
wandb:                   test_loss_step 74.53511
wandb:                train_loss/island 63.53263
wandb:                 train_loss/total 63.53263
wandb:                   train_step_acc 0.813
wandb:              trainer/global_step 2420
wandb:               val_last_step_loss 72.44647
wandb:                        valid_acc 0.5642
wandb: 
wandb: ğŸš€ View run exp_search_model_type_custom-resnet34_latent_size_3_i97l5jnx_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/i97l5jnx
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 214 media file(s), 2208 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240423_052214-i97l5jnx/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240423_095806-0w147xc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_custom-resnet34_latent_size_10_0w147xc9_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/0w147xc9
End of experiment: exp_search_model_type_custom-resnet34_latent_size_3; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: exp_search_model_type_custom-resnet34_latent_size_10; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=10, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='custom-resnet34', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: custom-resnet34
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: Generated:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_CustomResNet34 [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ CustomResNet34      â”‚ 21.3 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 21.3 M                                                                                                               
Non-trainable params: 0                                                                                                                
Total params: 21.3 M                                                                                                                   
Total estimated model params size (MB): 85                                                                                             
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
Epoch 0    â”                                        7/274 0:00:01 â€¢ 0:00:38 7.09it/s loss: 1.12e+30 v_num: 7xc9 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 68, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 758, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.044 MB of 0.044 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run exp_search_model_type_custom-resnet34_latent_size_10_0w147xc9_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/0w147xc9
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240423_095806-0w147xc9/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240423_095842-efgwfr98
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_custom-resnet34_latent_size_20_efgwfr98_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/efgwfr98
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: exp_search_model_type_custom-resnet34_latent_size_10; repeat 1/1
Clearing gpu cache and invoking garbage collector
[31mWARNING:	dreaming images were not flushed by wandb. [0m
Done
Running experiment: exp_search_model_type_custom-resnet34_latent_size_20; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='custom-resnet34', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: custom-resnet34
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_CustomResNet34 [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ CustomResNet34      â”‚ 21.3 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 21.3 M                                                                                                               
Non-trainable params: 0                                                                                                                
Total params: 21.3 M                                                                                                                   
Total estimated model params size (MB): 85                                                                                             
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
Epoch 0    â•¸                                        5/274 0:00:01 â€¢ 0:00:39 7.02it/s loss: 3.39e+24 v_num: fr98 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 68, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 758, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.039 MB uploadedwandb: / 0.017 MB of 0.039 MB uploadedwandb: - 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run exp_search_model_type_custom-resnet34_latent_size_20_efgwfr98_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/efgwfr98
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240423_095842-efgwfr98/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240423_095918-n1g2c05x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp_search_model_type_custom-resnet34_latent_size_30_n1g2c05x_latent-multitarget-multitask_dream_tr_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/n1g2c05x
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: exp_search_model_type_custom-resnet34_latent_size_20; repeat 1/1
Clearing gpu cache and invoking garbage collector
[31mWARNING:	dreaming images were not flushed by wandb. [0m
Done
Running experiment: exp_search_model_type_custom-resnet34_latent_size_30; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget-multitask', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], test_at=[True], save=Namespace(model=True, enable_checkpoint=False, dreams=True, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=2, schedule=[300, 0], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=[True], scale=0.1, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=[1, 2], clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=[True], scale=0.01), l2=Namespace(use_at=[True], coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='custom-resnet34', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=20.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=0.0, ratio_gamma=1.0, scale_gamma=1.0, ratio_milestones=None, scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=220, multitarget=Namespace(enable=True, random=False), batch_size=220, optim=Namespace(type='adam', kwargs=Namespace(lr=0.05, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[500], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=[False], enable_vis_at=[1], standard_image_size=[32], decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: SPLIT-CLASSIC-FILLLAST
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: custom-resnet34
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '0.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_CustomResNet34 [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mVIS: Enabled multitarget visualization. Each batch will have multiple targets. [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ CustomResNet34      â”‚ 21.3 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 21.3 M                                                                                                               
Non-trainable params: 0                                                                                                                
Total params: 21.3 M                                                                                                                   
Total estimated model params size (MB): 85                                                                                             
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
Epoch 0    â•¸                                        5/274 0:00:01 â€¢ 0:00:42 6.46it/s loss: 1.51e+33 v_num: c05x 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 68, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 758, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.039 MB uploadedwandb: / 0.017 MB of 0.039 MB uploadedwandb: - 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: ğŸš€ View run exp_search_model_type_custom-resnet34_latent_size_30_n1g2c05x_latent-multitarget-multitask_dream_tr_ at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02/runs/n1g2c05x
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-22%3D18-07-02
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240423_095918-n1g2c05x/logs
End of experiment: exp_search_model_type_custom-resnet34_latent_size_30; repeat 1/1
Clearing gpu cache and invoking garbage collector
[31mWARNING:	dreaming images were not flushed by wandb. [0m
Done
