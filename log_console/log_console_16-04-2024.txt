Experiments to be run:
* 
-d c10 --model.num_classes 10 --loop.schedule 200 --config.framework_type crossentropy-default --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 100 150 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --config.seed 2024 
* 
-d c100 --model.num_classes 100 --loop.schedule 200 --config.framework_type crossentropy-default --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 100 150 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --config.seed 2024 
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 3 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 3 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 3 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 3 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 3 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 3 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 3 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 3 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 3 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 10 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 10 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 10 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 10 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 10 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 10 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 10 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 10 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 10 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 20 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 20 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 20 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 20 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 20 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 20 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 20 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 20 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 20 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 80 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 220
Global seed set to 2024
wandb: Currently logged in as: 01133344 (cccvb). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240416_171530-1olzd0gv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_160_batch_size_220_1olzd0gv_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/1olzd0gv
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 120 --datamodule.batch_size 320
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 120
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 220
* 
-d c100 --model.num_classes 100 --loop.schedule 300 --config.framework_type latent-multitarget --model.type VGG --loop.num_loops 1 --loop.train_at 0 --model.optim.type sgd --model.optim.kwargs.lr 0.1 --model.sched.type MULTISTEP-SCHED --model.sched.kwargs.gamma 0.1 --model.sched.kwargs.milestones 140 180 --datamodule.num_workers 3 --loop.save.root model_save/test --loop.save.model --loop.load.root model_save/test --stat.collect_stats.enable --model.loss.chi.shift_min_distance 0 --model.loss.chi.ratio_gamma 2  --model.loss.chi.ratio_milestones 40 60 80 100 --config.seed 2024  --model.latent.size 30 --model.loss.chi.ratio 10 --model.loss.chi.scale 160 --datamodule.batch_size 320

Running experiment: crossentropy_default_c10_sgd; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 1).
Running experiment: crossentropy_default_c100_sgd; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 2).
Running experiment: chi_sqr_c100_sgd_search_latent_size_3_chi_ratio_10_chi_scale_80_batch_size_120; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 3).
Running experiment: chi_sqr_c100_sgd_search_latent_size_3_chi_ratio_10_chi_scale_80_batch_size_220; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 4).
Running experiment: chi_sqr_c100_sgd_search_latent_size_3_chi_ratio_10_chi_scale_80_batch_size_320; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 5).
Running experiment: chi_sqr_c100_sgd_search_latent_size_3_chi_ratio_10_chi_scale_120_batch_size_120; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 6).
Running experiment: chi_sqr_c100_sgd_search_latent_size_3_chi_ratio_10_chi_scale_120_batch_size_220; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 7).
Running experiment: chi_sqr_c100_sgd_search_latent_size_3_chi_ratio_10_chi_scale_120_batch_size_320; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 8).
Running experiment: chi_sqr_c100_sgd_search_latent_size_3_chi_ratio_10_chi_scale_160_batch_size_120; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 9).
Running experiment: chi_sqr_c100_sgd_search_latent_size_3_chi_ratio_10_chi_scale_160_batch_size_220; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 10).
Running experiment: chi_sqr_c100_sgd_search_latent_size_3_chi_ratio_10_chi_scale_160_batch_size_320; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 11).
Running experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_80_batch_size_120; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 12).
Running experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_80_batch_size_220; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 13).
Running experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_80_batch_size_320; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 14).
Running experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_120_batch_size_120; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 15).
Running experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_120_batch_size_220; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 16).
Running experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_120_batch_size_320; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 17).
Running experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_160_batch_size_120; repeat 1/1
Experiment skipped because of 'start_at' argument (19 > 18).
Running experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_160_batch_size_220; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=10, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=160.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mINFO: Generated current run name: d16-04-2024_h18-50-21_1olzd0gv [0m
[31mWARNING: At loop 1 selected last epoch per task "300" because list index out of range. [0m
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 274/274 0:00:19 â€¢ 0:00:00 13.92it/s loss: -9.75e+04 v_num: d0gv 
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of training.
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5881999731063843     â”‚
â”‚      test_loss_epoch      â”‚      -73233.4140625       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46 0:00:48 â€¢ 0:00:00 42.44it/s  
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 10 dimensions.
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h18-50-21_1olzd0gv/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h18-50-21_1olzd0gv/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h18-50-21_1olzd0gv/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h18-50-21_1olzd0gv/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h18-50-21_1olzd0gv/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h18-50-21_1olzd0gv/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h18-50-21_1olzd0gv/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h18-50-21_1olzd0gv/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h18-50-21_1olzd0gv/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h18-50-21_1olzd0gv/plots/mean_dist_matrix_idx100
wandb: - 4.026 MB of 6.163 MB uploadedwandb: \ 5.115 MB of 6.163 MB uploadedwandb: | 5.115 MB of 6.163 MB uploadedwandb: / 6.163 MB of 6.163 MB uploadedwandb: - 6.163 MB of 6.163 MB uploadedwandb: \ 6.163 MB of 6.163 MB uploadedwandb: | 6.303 MB of 6.549 MB uploaded (0.002 MB deduped)wandb: / 6.549 MB of 6.549 MB uploaded (0.002 MB deduped)wandb: - 6.549 MB of 6.549 MB uploaded (0.002 MB deduped)wandb: \ 6.549 MB of 6.549 MB uploaded (0.002 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          positive_loss â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–ˆâ–†â–…â–…â–„â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                  ratio â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: stats/collect_accuracy â–„â–ˆâ–â–ƒâ–„â–„â–ƒâ–ƒâ–…â–†â–†â–‡
wandb:     stats/collect_loss â–ƒâ–„â–ˆâ–‚â–„â–†â–…â–‡â–‚â–ƒâ–…â–
wandb:               test_acc â–
wandb:        test_loss_epoch â–
wandb:         test_loss_step â–„â–„â–‡â–ƒâ–„â–†â–…â–ƒâ–„â–…â–‚â–‡â–„â–„â–„â–ƒâ–ƒâ–ˆâ–†â–ˆâ–ƒâ–ƒâ–â–‚â–‡â–…â–‡â–…â–„â–‡â–„â–‡â–„â–‚â–â–‚â–‡â–…â–„â–‡
wandb:      train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–
wandb:     val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ƒâ–‚â–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              valid_acc â–â–â–â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                  epoch 300
wandb:          negative_loss -100821.94531
wandb:          positive_loss 5834.35205
wandb:                  ratio 160.0
wandb:            rho_sigma_2 25600.0
wandb:                  scale 160.0
wandb: stats/collect_accuracy 0.59508
wandb:     stats/collect_loss -78912.34375
wandb:               test_acc 0.5882
wandb:        test_loss_epoch -73233.41406
wandb:         test_loss_step -65892.60938
wandb:      train_loss/island -95601.64844
wandb:       train_loss/total -95601.64844
wandb:         train_step_acc 0.83338
wandb:    trainer/global_step 2420
wandb:     val_last_step_loss -73233.41406
wandb:              valid_acc 0.5882
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_160_batch_size_220_1olzd0gv_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/1olzd0gv
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 113 media file(s), 106 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240416_171530-1olzd0gv/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240416_185429-rip6vtgo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_160_batch_size_320_rip6vtgo_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/rip6vtgo
End of experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_160_batch_size_220; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_160_batch_size_320; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=10, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=160.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=320)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[34mINFO: Generated:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mINFO: Generated current run name: d16-04-2024_h20-14-11_rip6vtgo [0m
[31mWARNING: At loop 1 selected last epoch per task "300" because list index out of range. [0m
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 189/189 0:00:15 â€¢ 0:00:00 12.09it/s loss: -9.65e+04 v_num: vtgo 
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of training.
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5778999924659729     â”‚
â”‚      test_loss_epoch      â”‚       -71811.03125        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 32/32 0:00:46 â€¢ 0:00:00 36.94it/s  
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 10 dimensions.
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h20-14-11_rip6vtgo/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h20-14-11_rip6vtgo/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h20-14-11_rip6vtgo/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h20-14-11_rip6vtgo/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h20-14-11_rip6vtgo/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h20-14-11_rip6vtgo/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h20-14-11_rip6vtgo/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h20-14-11_rip6vtgo/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h20-14-11_rip6vtgo/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d16-04-2024_h20-14-11_rip6vtgo/plots/mean_dist_matrix_idx100
wandb: - 3.998 MB of 6.117 MB uploadedwandb: \ 6.117 MB of 6.117 MB uploadedwandb: | 6.117 MB of 6.117 MB uploadedwandb: / 6.117 MB of 6.117 MB uploadedwandb: - 6.117 MB of 6.117 MB uploadedwandb: \ 6.117 MB of 6.117 MB uploadedwandb: | 6.257 MB of 6.502 MB uploaded (0.005 MB deduped)wandb: / 6.257 MB of 6.502 MB uploaded (0.005 MB deduped)wandb: - 6.502 MB of 6.502 MB uploaded (0.005 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          positive_loss â–â–â–â–â–â–â–â–â–‚â–â–â–ƒâ–‚â–‚â–ˆâ–†â–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                  ratio â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: stats/collect_accuracy â–â–â–…â–…â–†â–…â–‡â–ˆ
wandb:     stats/collect_loss â–„â–ˆâ–…â–ƒâ–„â–ƒâ–ˆâ–
wandb:               test_acc â–
wandb:        test_loss_epoch â–
wandb:         test_loss_step â–ƒâ–†â–„â–ƒâ–„â–ƒâ–†â–â–„â–„â–„â–‚â–ƒâ–†â–†â–ƒâ–â–‚â–ƒâ–…â–„â–…â–ƒâ–…â–†â–‚â–â–‚â–…â–ƒâ–„â–ˆ
wandb:      train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_step_acc â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–…â–„â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–
wandb:     val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–„â–…â–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              valid_acc â–â–â–â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–„â–„â–„â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                  epoch 300
wandb:          negative_loss -99773.65625
wandb:          positive_loss 4388.5332
wandb:                  ratio 160.0
wandb:            rho_sigma_2 25600.0
wandb:                  scale 160.0
wandb: stats/collect_accuracy 0.57695
wandb:     stats/collect_loss -77478.3125
wandb:               test_acc 0.5779
wandb:        test_loss_epoch -71811.03125
wandb:         test_loss_step -62319.03516
wandb:      train_loss/island -95591.73438
wandb:       train_loss/total -95591.73438
wandb:         train_step_acc 0.83566
wandb:    trainer/global_step 2240
wandb:     val_last_step_loss -71811.03125
wandb:              valid_acc 0.5779
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_160_batch_size_320_rip6vtgo_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/rip6vtgo
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 113 media file(s), 106 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240416_185429-rip6vtgo/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240416_201825-kxohltra
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_120_kxohltra_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/kxohltra
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_10_chi_ratio_10_chi_scale_160_batch_size_320; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_120; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=80.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=120)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
Epoch 100 â•¸                                 10/501 0:00:00 â€¢ 0:00:27 18.43it/s loss: 4.38e+19 v_num: ltra 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.022 MB of 0.045 MB uploaded (0.005 MB deduped)wandb: - 0.038 MB of 0.045 MB uploaded (0.005 MB deduped)wandb: \ 0.038 MB of 0.045 MB uploaded (0.005 MB deduped)wandb: | 0.045 MB of 0.045 MB uploaded (0.005 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 10.2%             
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‡â–‡â–ˆâ–†â–†â–‡â–†â–†
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–‡â–†â–†â–†â–â–‚â–â–â–â–‚â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–‡â–†â–†â–†â–â–‚â–â–â–â–‚â–â–
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–ƒâ–„â–„â–…â–…â–…â–…â–†â–„â–…â–…â–†â–†â–‡â–‡â–‡â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†â–ƒâ–â–â–â–â–â–â–
wandb:           valid_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–ƒâ–„â–„â–…â–…â–…â–†â–…â–ƒâ–„â–…â–†â–…â–‡â–†â–‡â–„â–‡â–‡â–†â–‡â–‡â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:       negative_loss -99541.6875
wandb:       positive_loss 13073.72266
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 80.0
wandb:   train_loss/island -91371.39844
wandb:    train_loss/total -91371.39844
wandb:      train_step_acc 0.47664
wandb: trainer/global_step 41699
wandb:  val_last_step_loss -85643.84375
wandb:           valid_acc 0.4157
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_120_kxohltra_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/kxohltra
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240416_201825-kxohltra/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240416_210110-xko79idj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_220_xko79idj_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/xko79idj
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_120; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_220; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=80.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
Epoch 100 â”                                  9/274 0:00:01 â€¢ 0:00:24 11.42it/s loss: 1.04e+18 v_num: 9idj 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.036 MB uploadedwandb: / 0.015 MB of 0.038 MB uploadedwandb: - 0.015 MB of 0.038 MB uploadedwandb: \ 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–‡â–†â–…â–‡â–…â–…â–†
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†â–‚â–‚â–â–‚â–â–â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†â–‚â–‚â–â–‚â–â–â–â–
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–„â–„â–…â–…â–†â–†â–†â–†â–„â–…â–†â–†â–‡â–‡â–‡â–ˆâ–„â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–‡â–†â–†â–‡â–ƒâ–ƒâ–‚â–ƒâ–â–â–ƒâ–
wandb:           valid_acc â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–„â–„â–„â–„â–„â–…â–‚â–ƒâ–„â–ƒâ–†â–…â–†â–†â–‚â–„â–…â–‡â–…â–‡â–‡â–†â–ƒâ–„â–…â–…â–ˆâ–‡â–…â–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:       negative_loss -100259.03125
wandb:       positive_loss 10196.83887
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 80.0
wandb:   train_loss/island -93310.84375
wandb:    train_loss/total -93310.84375
wandb:      train_step_acc 0.51284
wandb: trainer/global_step 22799
wandb:  val_last_step_loss -75768.73438
wandb:           valid_acc 0.3365
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_220_xko79idj_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/xko79idj
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240416_210110-xko79idj/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240416_213351-c8d06f2k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_320_c8d06f2k_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/c8d06f2k
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_220; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_320; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=80.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=320)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
Epoch 100 â”â•¸                                 9/189 0:00:01 â€¢ 0:00:16 11.95it/s loss: 3.87e+19 v_num: 6f2k 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.038 MB uploadedwandb: | 0.015 MB of 0.038 MB uploadedwandb: / 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–‚â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–†â–‡â–†â–†â–„â–…â–…
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–‚â–‚â–‚â–â–â–â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–‚â–‚â–‚â–â–â–â–â–
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–ƒâ–„â–„â–…â–…â–…â–†â–†â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–„â–…â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–„â–…â–‚â–ƒâ–ƒâ–‚â–ƒâ–
wandb:           valid_acc â–â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–„â–ƒâ–‚â–„â–‚â–‚â–‚â–„â–„â–„â–„â–…â–‚â–ƒâ–ƒâ–„â–ƒâ–‡â–…â–…â–‚â–‚â–†â–„â–„â–†â–…â–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:       negative_loss -99454.84375
wandb:       positive_loss 9643.04492
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 80.0
wandb:   train_loss/island -93224.07031
wandb:    train_loss/total -93224.07031
wandb:      train_step_acc 0.52674
wandb: trainer/global_step 15699
wandb:  val_last_step_loss -78929.92969
wandb:           valid_acc 0.3979
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_320_c8d06f2k_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/c8d06f2k
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240416_213351-c8d06f2k/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240416_220047-evyozas9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_120_evyozas9_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/evyozas9
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_80_batch_size_320; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_120; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=120.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=120)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20f52329e0>
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1462, in _shutdown_workers
    if w.is_alive():
  File "/usr/lib/python3.10/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mINFO: Generated current run name: d17-04-2024_h00-07-39_evyozas9 [0m
[31mWARNING: At loop 1 selected last epoch per task "300" because list index out of range. [0m
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 501/501 0:00:25 â€¢ 0:00:00 19.98it/s loss: -3.99e+05 v_num: zas9 
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of training.
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5730999708175659     â”‚
â”‚      test_loss_epoch      â”‚       -358972.28125       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0:00:48 â€¢ 0:00:00 54.80it/s  
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
/home/ubuntu/continual_dreaming/stats/point_plot.py:369: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  fig.savefig(n)
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/wandb/sdk/data_types/image.py:302: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  util.ensure_matplotlib_figure(data).savefig(buf, format="png")
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 20 dimensions.
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h00-07-39_evyozas9/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h00-07-39_evyozas9/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h00-07-39_evyozas9/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h00-07-39_evyozas9/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h00-07-39_evyozas9/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h00-07-39_evyozas9/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h00-07-39_evyozas9/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h00-07-39_evyozas9/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h00-07-39_evyozas9/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h00-07-39_evyozas9/plots/mean_dist_matrix_idx100
wandb: - 5.975 MB of 6.246 MB uploadedwandb: \ 5.612 MB of 6.246 MB uploadedwandb: | 5.612 MB of 6.246 MB uploadedwandb: / 5.612 MB of 6.246 MB uploadedwandb: - 5.612 MB of 6.246 MB uploadedwandb: \ 6.386 MB of 6.627 MB uploaded (0.005 MB deduped)wandb: | 6.386 MB of 6.627 MB uploaded (0.005 MB deduped)wandb: / 6.627 MB of 6.627 MB uploaded (0.005 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          positive_loss â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–ˆâ–‡â–‡â–‡â–†â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–‚
wandb:                  ratio â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: stats/collect_accuracy â–‡â–ˆâ–†â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:     stats/collect_loss â–ƒâ–„â–„â–ˆâ–ˆâ–â–‚â–…â–„â–„â–‚â–‚â–†â–‡â–…â–â–‚â–„â–…â–„â–ƒ
wandb:               test_acc â–
wandb:        test_loss_epoch â–
wandb:         test_loss_step â–ƒâ–„â–‡â–‚â–„â–‚â–†â–…â–„â–„â–†â–‡â–‡â–…â–„â–‚â–‚â–‡â–ƒâ–„â–†â–„â–ƒâ–‚â–‡â–â–„â–„â–„â–‡â–ƒâ–†â–‚â–‚â–‚â–ƒâ–…â–†â–â–ˆ
wandb:      train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_step_acc â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–
wandb:     val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              valid_acc â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–„â–…â–†â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                  epoch 300
wandb:          negative_loss -416026.90625
wandb:          positive_loss 17307.42383
wandb:                  ratio 160.0
wandb:            rho_sigma_2 25600.0
wandb:                  scale 120.0
wandb: stats/collect_accuracy 0.58254
wandb:     stats/collect_loss -363746.25
wandb:               test_acc 0.5731
wandb:        test_loss_epoch -358972.28125
wandb:         test_loss_step -333628.375
wandb:      train_loss/island -402301.125
wandb:       train_loss/total -402301.125
wandb:         train_step_acc 0.72696
wandb:    trainer/global_step 2400
wandb:     val_last_step_loss -358972.28125
wandb:              valid_acc 0.5731
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_120_evyozas9_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/evyozas9
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 113 media file(s), 106 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240416_220047-evyozas9/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_001141-vo4vpi1r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_220_vo4vpi1r_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/vo4vpi1r
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_120; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_220; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=120.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
Epoch 100 â”â•¸                                15/274 0:00:01 â€¢ 0:00:20 13.48it/s loss: 4.75e+22 v_num: pi1r 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.038 MB uploadedwandb: | 0.015 MB of 0.038 MB uploadedwandb: / 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–†â–ˆâ–…â–…â–‡â–…â–…â–…
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–‡â–†â–†â–†â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–‡â–†â–†â–†â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–ƒâ–„â–„â–…â–…â–…â–…â–†â–„â–…â–…â–†â–†â–†â–‡â–‡â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–…â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–
wandb:           valid_acc â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–„â–„â–„â–„â–„â–‚â–ƒâ–„â–„â–„â–…â–„â–†â–‚â–…â–„â–…â–†â–…â–…â–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:       negative_loss -99375.125
wandb:       positive_loss 8257.77539
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 120.0
wandb:   train_loss/island -96563.01562
wandb:    train_loss/total -96563.01562
wandb:      train_step_acc 0.51614
wandb: trainer/global_step 22799
wandb:  val_last_step_loss -84095.91406
wandb:           valid_acc 0.4368
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_220_vo4vpi1r_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/vo4vpi1r
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_001141-vo4vpi1r/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_004351-xqho50b6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_320_xqho50b6_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/xqho50b6
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_220; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_320; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=120.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=320)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
Epoch 100 â”â”                                13/189 0:00:01 â€¢ 0:00:16 11.23it/s loss: 6.93e+22 v_num: 50b6 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.038 MB uploadedwandb: | 0.015 MB of 0.038 MB uploadedwandb: / 0.015 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‡â–ˆâ–‡â–‡â–†â–†â–†â–…
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–†â–‚â–‚â–â–â–â–â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–†â–‚â–‚â–â–â–â–â–â–
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–ƒâ–…â–…â–†â–†â–†â–‡â–‡â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–‡â–‡â–†â–â–‚â–â–‚â–â–‚â–‚
wandb:           valid_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–‚â–„â–ƒâ–„â–…â–„â–„â–‡â–â–„â–…â–‡â–†â–ˆâ–…â–†â–â–†â–…â–‡â–…â–ˆâ–†â–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:       negative_loss -99330.20312
wandb:       positive_loss 9154.17188
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 120.0
wandb:   train_loss/island -91871.90625
wandb:    train_loss/total -91871.90625
wandb:      train_step_acc 0.53312
wandb: trainer/global_step 15699
wandb:  val_last_step_loss -66347.35938
wandb:           valid_acc 0.2988
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_320_xqho50b6_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/xqho50b6
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_004351-xqho50b6/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_011127-8my0pyi7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_120_8my0pyi7_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/8my0pyi7
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_120_batch_size_320; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_120; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=160.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=120)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mINFO: Generated current run name: d17-04-2024_h03-18-42_8my0pyi7 [0m
[31mWARNING: At loop 1 selected last epoch per task "300" because list index out of range. [0m
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 501/501 0:00:25 â€¢ 0:00:00 19.61it/s loss: -3.95e+05 v_num: pyi7 
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of training.
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5539000034332275     â”‚
â”‚      test_loss_epoch      â”‚       -357942.1875        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0:00:47 â€¢ 0:00:00 54.13it/s  
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
/home/ubuntu/continual_dreaming/stats/point_plot.py:369: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  fig.savefig(n)
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/wandb/sdk/data_types/image.py:302: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  util.ensure_matplotlib_figure(data).savefig(buf, format="png")
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 20 dimensions.
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h03-18-42_8my0pyi7/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h03-18-42_8my0pyi7/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h03-18-42_8my0pyi7/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h03-18-42_8my0pyi7/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h03-18-42_8my0pyi7/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h03-18-42_8my0pyi7/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h03-18-42_8my0pyi7/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h03-18-42_8my0pyi7/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h03-18-42_8my0pyi7/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h03-18-42_8my0pyi7/plots/mean_dist_matrix_idx100
wandb: - 2.431 MB of 6.312 MB uploadedwandb: \ 2.431 MB of 6.312 MB uploadedwandb: | 6.312 MB of 6.312 MB uploadedwandb: / 6.312 MB of 6.312 MB uploadedwandb: - 6.312 MB of 6.312 MB uploadedwandb: \ 6.452 MB of 6.693 MB uploaded (0.005 MB deduped)wandb: | 6.693 MB of 6.693 MB uploaded (0.005 MB deduped)wandb: / 6.693 MB of 6.693 MB uploaded (0.005 MB deduped)wandb: - 6.693 MB of 6.693 MB uploaded (0.005 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          positive_loss â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ˆâ–‡â–†â–‡â–…â–„â–…â–…â–‚â–„â–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–…â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚
wandb:                  ratio â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: stats/collect_accuracy â–…â–ˆâ–‡â–ƒâ–â–â–ƒâ–‚â–â–â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:     stats/collect_loss â–„â–‚â–ƒâ–…â–ˆâ–â–ƒâ–…â–…â–…â–‚â–ƒâ–…â–†â–‚â–‚â–‚â–„â–„â–ƒâ–ƒ
wandb:               test_acc â–
wandb:        test_loss_epoch â–
wandb:         test_loss_step â–„â–ƒâ–ˆâ–ƒâ–…â–‚â–…â–‚â–„â–ƒâ–ƒâ–†â–…â–…â–„â–ƒâ–ƒâ–„â–â–„â–ƒâ–„â–ƒâ–ƒâ–†â–ƒâ–ƒâ–„â–…â–„â–„â–†â–‚â–â–‚â–ƒâ–…â–…â–‚â–ˆ
wandb:      train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–‡â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–‡â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_step_acc â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–
wandb:     val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              valid_acc â–â–â–â–‚â–‚â–ƒâ–‚â–„â–ƒâ–„â–…â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                  epoch 300
wandb:          negative_loss -416339.4375
wandb:          positive_loss 17348.42383
wandb:                  ratio 160.0
wandb:            rho_sigma_2 25600.0
wandb:                  scale 160.0
wandb: stats/collect_accuracy 0.55437
wandb:     stats/collect_loss -365271.0625
wandb:               test_acc 0.5539
wandb:        test_loss_epoch -357942.1875
wandb:         test_loss_step -323227.21875
wandb:      train_loss/island -390016.59375
wandb:       train_loss/total -390016.59375
wandb:         train_step_acc 0.70164
wandb:    trainer/global_step 2400
wandb:     val_last_step_loss -357942.1875
wandb:              valid_acc 0.5539
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_120_8my0pyi7_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/8my0pyi7
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 113 media file(s), 106 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_011127-8my0pyi7/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_032246-utkt6pba
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_220_utkt6pba_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/utkt6pba
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_120; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_220; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=160.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mINFO: Generated current run name: d17-04-2024_h04-57-13_utkt6pba [0m
[31mWARNING: At loop 1 selected last epoch per task "300" because list index out of range. [0m
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 274/274 0:00:18 â€¢ 0:00:00 14.66it/s loss: -4.03e+05 v_num: 6pba 
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of training.
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5799000263214111     â”‚
â”‚      test_loss_epoch      â”‚        -352306.125        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46/46 0:00:46 â€¢ 0:00:00 41.77it/s  
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
/home/ubuntu/continual_dreaming/stats/point_plot.py:369: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  fig.savefig(n)
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/wandb/sdk/data_types/image.py:302: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  util.ensure_matplotlib_figure(data).savefig(buf, format="png")
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 20 dimensions.
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h04-57-13_utkt6pba/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h04-57-13_utkt6pba/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h04-57-13_utkt6pba/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h04-57-13_utkt6pba/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h04-57-13_utkt6pba/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h04-57-13_utkt6pba/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h04-57-13_utkt6pba/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h04-57-13_utkt6pba/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h04-57-13_utkt6pba/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h04-57-13_utkt6pba/plots/mean_dist_matrix_idx100
wandb: - 6.134 MB of 6.370 MB uploadedwandb: \ 5.431 MB of 6.370 MB uploadedwandb: | 5.431 MB of 6.370 MB uploadedwandb: / 5.431 MB of 6.370 MB uploadedwandb: - 6.370 MB of 6.370 MB uploadedwandb: \ 6.370 MB of 6.370 MB uploadedwandb: | 6.510 MB of 6.751 MB uploaded (0.005 MB deduped)wandb: / 6.751 MB of 6.751 MB uploaded (0.005 MB deduped)wandb: - 6.751 MB of 6.751 MB uploaded (0.005 MB deduped)wandb: \ 6.751 MB of 6.751 MB uploaded (0.005 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          positive_loss â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ˆâ–‡â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚
wandb:                  ratio â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: stats/collect_accuracy â–ˆâ–…â–‚â–ƒâ–‚â–â–â–â–‚â–‚â–‚â–‚
wandb:     stats/collect_loss â–â–…â–ˆâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–â–„â–„â–
wandb:               test_acc â–
wandb:        test_loss_epoch â–
wandb:         test_loss_step â–ƒâ–…â–‡â–„â–„â–„â–„â–ƒâ–„â–„â–‚â–‡â–…â–„â–…â–ƒâ–ƒâ–‡â–…â–ˆâ–‚â–ƒâ–â–‚â–†â–„â–†â–†â–„â–‡â–…â–ˆâ–…â–ƒâ–â–ƒâ–…â–…â–„â–‡
wandb:      train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–
wandb:     val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‚â–‚â–„â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              valid_acc â–â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–…â–ƒâ–„â–…â–„â–„â–ƒâ–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                  epoch 300
wandb:          negative_loss -416436.28125
wandb:          positive_loss 9187.17383
wandb:                  ratio 160.0
wandb:            rho_sigma_2 25600.0
wandb:                  scale 160.0
wandb: stats/collect_accuracy 0.58447
wandb:     stats/collect_loss -362553.125
wandb:               test_acc 0.5799
wandb:        test_loss_epoch -352306.125
wandb:         test_loss_step -340432.59375
wandb:      train_loss/island -405421.1875
wandb:       train_loss/total -405421.1875
wandb:         train_step_acc 0.78892
wandb:    trainer/global_step 2420
wandb:     val_last_step_loss -352306.125
wandb:              valid_acc 0.5799
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_220_utkt6pba_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/utkt6pba
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 113 media file(s), 106 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_032246-utkt6pba/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_050118-22c5zws1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_320_22c5zws1_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/22c5zws1
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_220; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_320; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=20, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=160.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=320)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mINFO: Generated current run name: d17-04-2024_h06-20-21_22c5zws1 [0m
[31mWARNING: At loop 1 selected last epoch per task "300" because list index out of range. [0m
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 189/189 0:00:15 â€¢ 0:00:00 12.64it/s loss: -3.99e+05 v_num: zws1 
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of training.
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5605999827384949     â”‚
â”‚      test_loss_epoch      â”‚        -347264.125        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 32/32 0:00:45 â€¢ 0:00:00 34.35it/s  
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
/home/ubuntu/continual_dreaming/stats/point_plot.py:369: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  fig.savefig(n)
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/wandb/sdk/data_types/image.py:302: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  util.ensure_matplotlib_figure(data).savefig(buf, format="png")
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 20 dimensions.
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h06-20-21_22c5zws1/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h06-20-21_22c5zws1/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h06-20-21_22c5zws1/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h06-20-21_22c5zws1/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h06-20-21_22c5zws1/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h06-20-21_22c5zws1/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h06-20-21_22c5zws1/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h06-20-21_22c5zws1/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h06-20-21_22c5zws1/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h06-20-21_22c5zws1/plots/mean_dist_matrix_idx100
wandb: - 4.227 MB of 6.439 MB uploadedwandb: \ 2.431 MB of 6.439 MB uploadedwandb: | 2.431 MB of 6.439 MB uploadedwandb: / 2.431 MB of 6.439 MB uploadedwandb: - 6.439 MB of 6.439 MB uploadedwandb: \ 6.439 MB of 6.439 MB uploadedwandb: | 6.579 MB of 6.820 MB uploaded (0.005 MB deduped)wandb: / 6.579 MB of 6.820 MB uploaded (0.005 MB deduped)wandb: - 6.579 MB of 6.820 MB uploaded (0.005 MB deduped)wandb: \ 6.820 MB of 6.820 MB uploaded (0.005 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          positive_loss â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ˆâ–„â–„â–„â–„â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                  ratio â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: stats/collect_accuracy â–ˆâ–â–‚â–„â–‚â–„â–…â–…
wandb:     stats/collect_loss â–ƒâ–ˆâ–ƒâ–â–†â–â–†â–‚
wandb:               test_acc â–
wandb:        test_loss_epoch â–
wandb:         test_loss_step â–ƒâ–†â–ƒâ–‚â–…â–‚â–…â–‚â–†â–…â–…â–ƒâ–‚â–ƒâ–†â–‚â–‚â–‚â–â–„â–…â–„â–ƒâ–„â–†â–â–‚â–‚â–…â–ƒâ–„â–ˆ
wandb:      train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_step_acc â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–„â–…â–„â–…â–…â–ƒâ–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–
wandb:     val_last_step_loss â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–†â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              valid_acc â–â–â–â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–„â–…â–â–ƒâ–…â–‚â–„â–„â–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                  epoch 300
wandb:          negative_loss -411735.0625
wandb:          positive_loss 13107.50977
wandb:                  ratio 160.0
wandb:            rho_sigma_2 25600.0
wandb:                  scale 160.0
wandb: stats/collect_accuracy 0.56055
wandb:     stats/collect_loss -353579.09375
wandb:               test_acc 0.5606
wandb:        test_loss_epoch -347264.125
wandb:         test_loss_step -326972.34375
wandb:      train_loss/island -398730.78125
wandb:       train_loss/total -398730.78125
wandb:         train_step_acc 0.76702
wandb:    trainer/global_step 2240
wandb:     val_last_step_loss -347264.125
wandb:              valid_acc 0.5606
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_320_22c5zws1_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/22c5zws1
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 113 media file(s), 106 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_050118-22c5zws1/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_062427-k03bkepd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_120_k03bkepd_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/k03bkepd
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_20_chi_ratio_10_chi_scale_160_batch_size_320; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_120; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=80.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=120)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mINFO: Generated current run name: d17-04-2024_h08-29-43_k03bkepd [0m
[31mWARNING: At loop 1 selected last epoch per task "300" because list index out of range. [0m
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 501/501 0:00:24 â€¢ 0:00:00 20.48it/s loss: 8.49e+08 v_num: kepd 
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of training.
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚   0.009999999776482582    â”‚
â”‚      test_loss_epoch      â”‚        74518848.0         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0:00:49 â€¢ 0:00:00 55.50it/s  
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
/home/ubuntu/continual_dreaming/stats/point_plot.py:369: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  fig.savefig(n)
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/wandb/sdk/data_types/image.py:302: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  util.ensure_matplotlib_figure(data).savefig(buf, format="png")
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 30 dimensions.
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h08-29-43_k03bkepd/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h08-29-43_k03bkepd/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h08-29-43_k03bkepd/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h08-29-43_k03bkepd/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h08-29-43_k03bkepd/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h08-29-43_k03bkepd/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h08-29-43_k03bkepd/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h08-29-43_k03bkepd/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h08-29-43_k03bkepd/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h08-29-43_k03bkepd/plots/mean_dist_matrix_idx100
wandb: - 3.786 MB of 3.846 MB uploadedwandb: \ 3.786 MB of 3.846 MB uploadedwandb: | 3.846 MB of 3.846 MB uploadedwandb: / 3.846 MB of 3.846 MB uploadedwandb: - 3.846 MB of 3.846 MB uploadedwandb: \ 3.846 MB of 3.846 MB uploadedwandb: | 3.985 MB of 4.226 MB uploaded (0.005 MB deduped)wandb: / 3.985 MB of 4.226 MB uploaded (0.005 MB deduped)wandb: - 4.226 MB of 4.226 MB uploaded (0.005 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          negative_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–†â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–†â–â–â–â–â–â–â–ˆ
wandb:          positive_loss â–â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–ˆâ–‡â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                  ratio â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: stats/collect_accuracy â–â–†â–ˆâ–†â–…â–…â–…â–†â–†â–‡â–†â–‡â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡
wandb:     stats/collect_loss â–‚â–â–â–‚â–â–‚â–â–‚â–‚â–â–‚â–ˆâ–â–‚â–‚â–â–â–‚â–‚â–‚â–‚
wandb:               test_acc â–
wandb:        test_loss_epoch â–
wandb:         test_loss_step â–ƒâ–â–â–â–â–ˆâ–â–â–ƒâ–â–â–â–â–†â–â–ƒâ–ƒâ–ƒâ–â–ƒâ–â–†â–â–â–ƒâ–â–ƒâ–†â–â–ˆâ–â–â–ƒâ–â–ƒâ–ƒâ–â–†â–â–
wandb:      train_loss/island â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–„â–â–â–â–â–â–â–â–â–â–â–
wandb:       train_loss/total â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–„â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_step_acc â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–†â–‡â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–
wandb:     val_last_step_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–‚â–â–‚â–â–‚â–â–‚â–â–â–â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–â–â–
wandb:              valid_acc â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–…â–†â–†â–‡â–†â–‡â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                  epoch 300
wandb:          negative_loss 1095264512.0
wandb:          positive_loss 161.18095
wandb:                  ratio 160.0
wandb:            rho_sigma_2 25600.0
wandb:                  scale 80.0
wandb: stats/collect_accuracy 0.00913
wandb:     stats/collect_loss 759690112.0
wandb:               test_acc 0.01
wandb:        test_loss_epoch 74518848.0
wandb:         test_loss_step 4126393.75
wandb:      train_loss/island 4126393.25
wandb:       train_loss/total 4126393.25
wandb:         train_step_acc 0.00982
wandb:    trainer/global_step 2400
wandb:     val_last_step_loss 74518848.0
wandb:              valid_acc 0.01
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_120_k03bkepd_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/k03bkepd
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 113 media file(s), 106 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_062427-k03bkepd/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_083400-triy772x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_220_triy772x_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/triy772x
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_120; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_220; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=80.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
Epoch 80  â”â”â”â•¸                              33/274 0:00:02 â€¢ 0:00:20 12.17it/s loss: 1.21e+22 v_num: 772x 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.037 MB uploadedwandb: / 0.015 MB of 0.037 MB uploadedwandb: - 0.037 MB of 0.037 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–â–â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–ˆâ–†â–†â–†â–…â–†â–…â–…â–†
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–‚â–â–â–â–â–â–â–â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–‚â–â–â–â–â–â–â–â–â–
wandb:      train_step_acc â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–‡â–‡â–†â–†â–‡â–„â–ƒâ–‚â–â–ƒâ–â–‚â–â–â–
wandb:           valid_acc â–â–‚â–â–â–â–‚â–‚â–â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–„â–…â–ƒâ–„â–ƒâ–ƒâ–„â–†â–†â–†â–†â–‡â–‡â–†â–‚â–ƒâ–…â–‡â–ƒâ–‡â–…â–ˆâ–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:               epoch 80
wandb:       negative_loss -50274.79688
wandb:       positive_loss 9442.45898
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 80.0
wandb:   train_loss/island -36694.78125
wandb:    train_loss/total -36694.78125
wandb:      train_step_acc 0.47066
wandb: trainer/global_step 18249
wandb:  val_last_step_loss -38077.28125
wandb:           valid_acc 0.2842
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_220_triy772x_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/triy772x
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_083400-triy772x/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_085952-0vgxjcam
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_320_0vgxjcam_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/0vgxjcam
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_220; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_320; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=80.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=320)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
Epoch 80  â”â”â”â”â•¸                             26/189 0:00:02 â€¢ 0:00:16 10.70it/s loss: 2.04e+21 v_num: jcam 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.037 MB uploadedwandb: | 0.015 MB of 0.037 MB uploadedwandb: / 0.037 MB of 0.037 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–â–â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–ˆâ–‡â–†â–†â–…â–†â–…â–†â–…
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‚â–‚â–â–â–â–â–â–â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‚â–‚â–â–â–â–â–â–â–â–
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–„â–ƒâ–ƒâ–…â–‚â–ƒâ–â–„â–â–
wandb:           valid_acc â–â–â–â–â–‚â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–„â–„â–…â–„â–‚â–„â–‚â–…â–…â–ƒâ–…â–ƒâ–†â–…â–â–ƒâ–ƒâ–‚â–…â–„â–ˆâ–ƒâ–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:               epoch 79
wandb:       negative_loss -47514.98828
wandb:       positive_loss 3769.29639
wandb:               ratio 40.0
wandb:         rho_sigma_2 1600.0
wandb:               scale 80.0
wandb:   train_loss/island -44767.64453
wandb:    train_loss/total -44767.64453
wandb:      train_step_acc 0.4776
wandb: trainer/global_step 12559
wandb:  val_last_step_loss -37768.07812
wandb:           valid_acc 0.3077
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_320_0vgxjcam_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/0vgxjcam
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_085952-0vgxjcam/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_092259-p6559ypo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_120_p6559ypo_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/p6559ypo
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_80_batch_size_320; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_120; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=120.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=120)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
Epoch 100 â•¸                                 12/501 0:00:00 â€¢ 0:00:24 20.61it/s loss: 1.24e+28 v_num: 9ypo 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.038 MB uploadedwandb: / 0.015 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–†â–ˆâ–†â–…â–†â–†â–†â–†
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–‚â–â–â–â–â–â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–‚â–â–â–â–â–â–â–
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–ƒâ–„â–„â–…â–…â–…â–…â–†â–„â–…â–…â–†â–†â–†â–‡â–‡â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–‚â–â–â–â–â–â–â–
wandb:           valid_acc â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–ƒâ–ƒâ–„â–…â–…â–…â–†â–…â–„â–…â–…â–†â–‡â–†â–‡â–‡â–„â–†â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:       negative_loss -195966.64062
wandb:       positive_loss 13833.76855
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 120.0
wandb:   train_loss/island -171484.92188
wandb:    train_loss/total -171484.92188
wandb:      train_step_acc 0.46464
wandb: trainer/global_step 41699
wandb:  val_last_step_loss -170077.60938
wandb:           valid_acc 0.406
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_120_p6559ypo_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/p6559ypo
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_092259-p6559ypo/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_100537-awocu41k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_220_awocu41k_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/awocu41k
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_120; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_220; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=120.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
Epoch 100 â”â•¸                                14/274 0:00:01 â€¢ 0:00:18 14.48it/s loss: 2.49e+18 v_num: u41k 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.038 MB uploadedwandb: / 0.015 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–†â–…â–…â–‡â–…â–…â–„
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–‚â–â–â–â–â–â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–‚â–â–â–â–â–â–â–
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–ƒâ–„â–„â–„â–…â–…â–…â–†â–„â–…â–†â–†â–†â–†â–‡â–‡â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–ˆâ–†â–‡â–†â–‡â–„â–ƒâ–‚â–‚â–â–‚â–ƒâ–
wandb:           valid_acc â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–„â–…â–…â–„â–‚â–ƒâ–…â–‚â–…â–„â–†â–…â–‚â–ƒâ–„â–…â–‡â–†â–„â–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:       negative_loss -195937.75
wandb:       positive_loss 12110.08984
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 120.0
wandb:   train_loss/island -169777.0625
wandb:    train_loss/total -169777.0625
wandb:      train_step_acc 0.50316
wandb: trainer/global_step 22799
wandb:  val_last_step_loss -168701.29688
wandb:           valid_acc 0.4008
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_220_awocu41k_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/awocu41k
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_100537-awocu41k/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_103747-723j18ey
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_320_723j18ey_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/723j18ey
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_220; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_320; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=120.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=320)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
Epoch 100 â”â•¸                                11/189 0:00:01 â€¢ 0:00:16 11.82it/s loss: 1.56e+19 v_num: 18ey 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.036 MB uploadedwandb: | 0.038 MB of 0.038 MB uploadedwandb: / 0.038 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–‚â–â–â–â–â–â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–‚â–â–â–â–â–â–â–
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–ƒâ–„â–„â–„â–…â–…â–…â–†â–ƒâ–…â–…â–†â–†â–†â–‡â–‡â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–‡â–†â–‡â–†â–†â–†â–‚â–ƒâ–â–ƒâ–‚â–â–â–
wandb:           valid_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–…â–‚â–ƒâ–„â–…â–ƒâ–ƒâ–†â–†â–‚â–…â–„â–†â–…â–‡â–ˆâ–‡â–…â–ƒâ–‡â–„â–…â–†â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:       negative_loss -192347.95312
wandb:       positive_loss 13892.14746
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 120.0
wandb:   train_loss/island -167273.8125
wandb:    train_loss/total -167273.8125
wandb:      train_step_acc 0.48878
wandb: trainer/global_step 15699
wandb:  val_last_step_loss -145758.32812
wandb:           valid_acc 0.2779
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_320_723j18ey_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/723j18ey
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_103747-723j18ey/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_110459-0m549hek
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_120_0m549hek_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/0m549hek
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_120_batch_size_320; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_120; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=160.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=120)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
[34mChanged learning rate from: '[0.1]' to: '[0.010000000000000002]' for optimizer index: 0 at epoch: 139 [0m
[34mChanged learning rate from: '[0.010000000000000002]' to: '[0.0010000000000000002]' for optimizer index: 0 at epoch: 179 [0m
[34mScheduler restarted at epoch 299 end. Learning rate: [0.0010000000000000002] [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
[34mENDING TASK 0, loop 0 [0m
[34mINFO: Generated current run name: d17-04-2024_h13-10-34_0m549hek [0m
[31mWARNING: At loop 1 selected last epoch per task "300" because list index out of range. [0m
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 501/501 0:00:24 â€¢ 0:00:00 20.91it/s loss: -7.72e+05 v_num: 9hek 
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of training.
Files already downloaded and verified
[95mINFO: Testing for classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test_acc          â”‚    0.5511000156402588     â”‚
â”‚      test_loss_epoch      â”‚        -713594.625        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 84/84 0:00:49 â€¢ 0:00:00 56.04it/s  
[34mINFO: COLLECT STATS: Selected normal dataset. [0m
[34mSTATISTICS: Collecting model stats [0m
[34mINFO: Selected None datasampler [0m
/home/ubuntu/continual_dreaming/stats/point_plot.py:369: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  fig.savefig(n)
/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/wandb/sdk/data_types/image.py:302: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  util.ensure_matplotlib_figure(data).savefig(buf, format="png")
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

WARNING: Plot 3D only for 3 dimensional space! Found 30 dimensions.
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h13-10-34_0m549hek/plots/std-mean_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h13-10-34_0m549hek/plots/distance_class_idx15
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h13-10-34_0m549hek/plots/distance_class_idx31
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h13-10-34_0m549hek/plots/distance_class_idx47
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h13-10-34_0m549hek/plots/distance_class_idx63
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h13-10-34_0m549hek/plots/distance_class_idx79
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h13-10-34_0m549hek/plots/distance_class_idx95
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h13-10-34_0m549hek/plots/distance_class_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h13-10-34_0m549hek/plots/mean_distance_idx100
INFO: Plot model_save/test/ClLatentChi/VGG/model_optim_type-sgd/d17-04-2024_h13-10-34_0m549hek/plots/mean_dist_matrix_idx100
wandb: - 4.406 MB of 6.648 MB uploadedwandb: \ 4.406 MB of 6.648 MB uploadedwandb: | 6.648 MB of 6.648 MB uploadedwandb: / 6.648 MB of 6.648 MB uploadedwandb: - 6.648 MB of 6.648 MB uploadedwandb: \ 6.648 MB of 6.648 MB uploadedwandb: | 6.648 MB of 6.648 MB uploadedwandb: / 6.787 MB of 7.028 MB uploaded (0.005 MB deduped)wandb: - 7.028 MB of 7.028 MB uploaded (0.005 MB deduped)wandb: \ 7.028 MB of 7.028 MB uploaded (0.005 MB deduped)wandb: | 7.028 MB of 7.028 MB uploaded (0.005 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          positive_loss â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–‡â–ˆâ–‡â–†â–†â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„
wandb:                  ratio â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: stats/collect_accuracy â–ˆâ–‡â–…â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–ƒâ–ƒâ–ƒâ–ƒ
wandb:     stats/collect_loss â–„â–ƒâ–…â–‡â–ˆâ–â–ƒâ–…â–ƒâ–„â–„â–…â–†â–…â–†â–â–â–„â–†â–ƒâ–
wandb:               test_acc â–
wandb:        test_loss_epoch â–
wandb:         test_loss_step â–ƒâ–„â–†â–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–‡â–†â–„â–…â–‚â–ƒâ–†â–‚â–ƒâ–„â–‚â–‚â–‚â–„â–ƒâ–ƒâ–†â–„â–‡â–„â–†â–â–‚â–‚â–ƒâ–„â–„â–‚â–ˆ
wandb:      train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_step_acc â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–
wandb:     val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              valid_acc â–â–â–â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–„â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:                  epoch 300
wandb:          negative_loss -800281.4375
wandb:          positive_loss 43053.55078
wandb:                  ratio 160.0
wandb:            rho_sigma_2 25600.0
wandb:                  scale 160.0
wandb: stats/collect_accuracy 0.56071
wandb:     stats/collect_loss -735968.875
wandb:               test_acc 0.5511
wandb:        test_loss_epoch -713594.625
wandb:         test_loss_step -657557.75
wandb:      train_loss/island -770127.875
wandb:       train_loss/total -770127.875
wandb:         train_step_acc 0.70798
wandb:    trainer/global_step 2400
wandb:     val_last_step_loss -713594.625
wandb:              valid_acc 0.5511
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_120_0m549hek_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/0m549hek
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 113 media file(s), 106 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_110459-0m549hek/logs
Global seed set to 2024
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_131458-axpzo6x0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_220_axpzo6x0_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/axpzo6x0
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_120; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_220; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=160.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=220)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
Epoch 100 â”â•¸                                16/274 0:00:01 â€¢ 0:00:19 14.31it/s loss: 7.09e+18 v_num: o6x0 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.035 MB uploadedwandb: | 0.015 MB of 0.038 MB uploadedwandb: / 0.015 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–†â–…â–…â–‡â–†â–…â–„
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–‚â–â–â–â–â–â–â–‚
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–‚â–â–â–â–â–â–â–‚
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–…â–…â–…â–…â–„â–…â–…â–…â–†â–†â–†â–‡â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–‡â–‡â–‡â–†â–‡â–ƒâ–‚â–‚â–ƒâ–â–‚â–‚â–‚
wandb:           valid_acc â–â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–„â–„â–…â–„â–„â–‚â–„â–†â–„â–ƒâ–‚â–†â–„â–‚â–…â–…â–…â–ˆâ–†â–†â–…
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:       negative_loss -192367.92188
wandb:       positive_loss 13132.35547
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 160.0
wandb:   train_loss/island -163392.54688
wandb:    train_loss/total -163392.54688
wandb:      train_step_acc 0.456
wandb: trainer/global_step 22799
wandb:  val_last_step_loss -142599.26562
wandb:           valid_acc 0.2145
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_220_axpzo6x0_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/axpzo6x0
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_131458-axpzo6x0/logs
Global seed set to 2024
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in log_run/wandb/run-20240417_134646-sega4bzo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_320_sega4bzo_latent-multitarget_dull_
wandb: â­ï¸ View project at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: ğŸš€ View run at https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/sega4bzo
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
End of experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_220; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
Running experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_320; repeat 1/1
Seed used: 2024
[34mINFO: Selected None datasampler [0m
[34m	Input command line: [0m
main_experiments.py --project_name exp_2024-04-15-10-02-35_VGG --start_at 19
[34m	Used config: [0m
{'pca_estimate_rank': 6, 'wandb': "Namespace(run=Namespace(folder='log_run/'), watch=Namespace(enable=False, log_freq=1000))", 'config': "Namespace(seed=2024, folder='run_conf/', load=None, export=None, test=Namespace(disable=False), cpu=False, dataset='c100', datasampler_type='none', num_tasks=1, framework_type='latent-multitarget', dream_obj_type=None, select_task_type=None, target_processing_type=None, task_split_type=None, overlay_type=None, split=Namespace(num_classes=None))", 'loop': "Namespace(train_at=[0], save=Namespace(model=True, enable_checkpoint=False, dreams=False, root='model_save/test', layer_stats=False, ignore_config=False), load=Namespace(model=False, dreams=False, root='model_save/test', id=None, name=None, layer_stats=False), num_loops=1, schedule=[300], model=Namespace(reload_at=None, reinit_at=None), weight_reset_sanity_check=False, layer_stats=Namespace(use_at=None, hook_to=None, device='cuda', flush_to_disk=False, type=None), vis=Namespace(layerloss=Namespace(mean_norm=Namespace(use_at=None, hook_to=None, scale=0.001, del_cov_after=False, device='cuda'), grad_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), grad_activ_pruning=Namespace(hook_to=None, use_at=None, percent=0.01, device='cuda'), deep_inversion=Namespace(use_at=None, scale=1.0, scale_file=None, hook_to=None), deep_inversion_target=Namespace(use_at=None, scale=1.0, hook_to=None)), generate_at=False, clear_dataset_at=None, image_reg=Namespace(var=Namespace(use_at=None, scale=2.5e-05), l2=Namespace(use_at=None, coeff=1e-05))))", 'model': "Namespace(latent=Namespace(buffer=Namespace(size_per_class=40), size=30, onehot=Namespace(type='diagonal')), num_classes=100, optim=Namespace(type='sgd', reset_type='default', kwargs=Namespace(lr=0.1, gamma=1, momentum=0, dampening=0, weight_decay=0, betas=[0.9, 0.999], amsgrad=False)), sched=Namespace(type='MULTISTEP-SCHED', kwargs=Namespace(gamma=0.1, milestones=[140, 180]), steps=(3,)), norm_lambda=0.0, type='VGG', default_weights=False, train_sanity_check=False, layer_replace=Namespace(enable=False), loss=Namespace(chi=Namespace(sigma=0.1, ratio=10.0, scale=160.0, l2=0.001, shift_min_distance=0.0, shift_std_of_mean=15.0, ratio_gamma=2.0, scale_gamma=1.0, ratio_milestones=[40.0, 60.0, 80.0, 100.0], scale_milestones=None, dual=Namespace(inner_scale=1.0, outer_scale=1.0))))", 'datamodule': "Namespace(disable_shuffle=False, num_workers=3, vis=Namespace(num_workers=None, per_target=128, multitarget=Namespace(enable=False, random=False), batch_size=128, optim=Namespace(type='adam', kwargs=Namespace(lr=0.001, betas=[0.9, 0.999], gamma=1, weight_decay=0, amsgrad=False, momentum=0, dampening=0)), sched=Namespace(type=None), threshold=[512], disable_transforms=False, disable_shuffle=False, image_type='pixel', only_vis_at=False, standard_image_size=None, decorrelate=False), test_num_workers=None, val_num_workers=None, batch_size=320)", 'fast_dev_run': 'Namespace(enable=False, batches=30, epochs=1, vis_threshold=[5])', 'stat': 'Namespace(compare_latent=False, disorder_dream=False, limit_plots_to=6, collect_stats=Namespace(enable=True, use_dream_dataset=False), plot_classes=None, disorder=Namespace(sigma=0.0, start_img_val=None), collect=Namespace(latent_buffer=Namespace(enable=False, name=None, cl_idx=None, size=50), single_dream=Namespace(enable=False, sigma=0.0)))', 'Plain args': "['main_experiments.py', '--project_name', 'exp_2024-04-15-10-02-35_VGG', '--start_at', '19']"}
[34mSelected configuration:[0m
	SELECT TASK: SELECT-CLASSIC
	TARGET PROCESSING: TARGET-LATENT-SAMPLE-NORMAL-MINIMAL-STD-MULTITARGET
	TASK SPLIT: NO-SPLIT
	DREAM OBJECTIVE: OBJECTIVE-LATENT-LOSSF-MULTITARGET-CREATOR
	MODEL TYPE: VGG
	OVERLAY TYPE: CL-MODEL-ISLAND [0m
[36mCHI-LOSS: Used buffer: CyclicBufferByClass() [0m
[34mINFO: Generating means shifts [0m
[34mINFO: shift_min_distance '0.0' or shift_std_of_mean '15.0' iss zero. Generating matrix of zeros. [0m
[34mINFO: Generated:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) [0m
[32mINFO: Using loss CHI_LOSS [0m
[32mMODEL TYPE: ClLatentChi_CLModel_VGG [0m
[36mVIS: Selected dream image type: pixel [0m
[95mTrain task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[95mValidation task split: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]] [0m
[31mFast dev run is False [0m
Files already downloaded and verified
[34mINFO: Created [95m0[34m optim config: [32mSGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
) [0m
[34mINFO: Created [95m0[34m sched config: [32mMultiStepLR
{'gamma': 0.1, 'milestones': [140, 180]} [0m
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name                 â”ƒ Type                â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ train_acc            â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 1 â”‚ train_acc_dream      â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 2 â”‚ _valid_accs          â”‚ ModuleDict          â”‚      0 â”‚
â”‚ 3 â”‚ test_acc             â”‚ MulticlassAccuracy  â”‚      0 â”‚
â”‚ 4 â”‚ model                â”‚ VGG                 â”‚  9.2 M â”‚
â”‚ 5 â”‚ cyclic_latent_buffer â”‚ CyclicBufferByClass â”‚      0 â”‚
â”‚ 6 â”‚ _loss_f              â”‚ ChiLoss             â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 9.2 M                                                                                   
Non-trainable params: 0                                                                                   
Total params: 9.2 M                                                                                       
Total estimated model params size (MB): 36                                                                
[34mSTARTING TASK 0, loop 0 -- classes in task [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[95mSelected task number: 0 [0m
[34mINFO: HOOKING UP NORMAL LOOP [0m
[95mINFO: Selected classes for normal dataloader: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] [0m
[31mWARNING:	dreaming images were not flushed by wandb. [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 10.0 to: 20.0 at step: 40 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 20.0 to: 40.0 at step: 60 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 40.0 to: 80.0 at step: 80 [0m
[34mINFO: Scheduler for ChiLoss ratio changed value from: 80.0 to: 160.0 at step: 100 [0m
Epoch 100 â”â”                                 12/189 0:00:01 â€¢ 0:00:16 11.18it/s loss: 7.5e+20 v_num: 4bzo 
Sleep 10 seconds...
Experiment Exception occurred
Traceback (most recent call last):
  File "/home/ubuntu/continual_dreaming/main_experiments.py", line 66, in main
    logic(args_exp, True, project_name=project_name, run_name=k)
  File "/home/ubuntu/continual_dreaming/latent_dreams.py", line 334, in logic
    trainer.fit(model, datamodule=cl_data_module)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 723, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1236, in _run
    results = self._run_stage()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1323, in _run_stage
    return self._run_train()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1353, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/executor/cl_loop.py", line 751, in advance
    self.custom_advance_f(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1595, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1646, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 155, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/torch/optim/sgd.py", line 66, in step
    loss = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 140, in _wrap_closure
    closure_result = closure()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1765, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/pythonEnv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 170, in training_step
    return super().training_step(batch=batch, batch_idx=batch_idx, optimizer_idx=optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_base.py", line 114, in training_step
    return self.training_step_normal(batch["normal"], optimizer_idx)
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_model.py", line 166, in training_step_normal
    loss = self.process_losses_normal(
  File "/home/ubuntu/continual_dreaming/model/overlay/cl_latent_chi.py", line 74, in process_losses_normal
    loss = self._loss_f(latent, y)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 304, in __call__
    super().__call__(input, target, train=train)
  File "/home/ubuntu/continual_dreaming/loss_function/chiLoss.py", line 79, in __call__
    assert not torch.any(torch.isnan(input)), f"Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters \
AssertionError: Input for chi-square loss is NaN. Try changing 'scale' >> 'ratio' hyperparameters or give bigger batch size (for X classes the X * 3.2 batch size should work good).

wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.035 MB uploadedwandb: | 0.030 MB of 0.038 MB uploadedwandb: / 0.030 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       negative_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:       positive_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ˆâ–‡â–†â–†â–…â–…â–…â–…
wandb:               ratio â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         rho_sigma_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   train_loss/island â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:    train_loss/total â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–
wandb:      train_step_acc â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–„â–…â–…â–…â–†â–†â–†â–‡â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_last_step_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–†â–‡â–†â–†â–„â–„â–‚â–†â–ƒâ–â–â–‚
wandb:           valid_acc â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–‚â–ƒâ–ƒâ–â–…â–„â–†â–†â–‚â–ƒâ–„â–‚â–ƒâ–‡â–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:               epoch 99
wandb:       negative_loss -191527.39062
wandb:       positive_loss 13666.8418
wandb:               ratio 80.0
wandb:         rho_sigma_2 6400.0
wandb:               scale 160.0
wandb:   train_loss/island -167549.375
wandb:    train_loss/total -167549.375
wandb:      train_step_acc 0.45194
wandb: trainer/global_step 15699
wandb:  val_last_step_loss -142171.78125
wandb:           valid_acc 0.2685
wandb: 
wandb: ğŸš€ View run chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_320_sega4bzo_latent-multitarget_dull_ at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG/runs/sega4bzo
wandb: â­ï¸ View project at: https://wandb.ai/cccvb/exp_2024-04-15-10-02-35_VGG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: log_run/wandb/run-20240417_134646-sega4bzo/logs
End of experiment: chi_sqr_c100_sgd_search_latent_size_30_chi_ratio_10_chi_scale_160_batch_size_320; repeat 1/1
Clearing gpu cache and invoking garbage collector
Done
